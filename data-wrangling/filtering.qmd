---
title: "Filtering"
---
```{r, setup, include=FALSE}
source("setup.R")
```

```{python}
#| echo: false
#| message: false
orders = r.orders
products = r.products
customers = r.customers
suppliers = r.suppliers
```

Filtering dataframes is a crucial skill of any data scientist and there is probably the most used method in data wrangling.

## Basic filtering techniques (`==`, `<`. `!=` etc.) 
The most basic form of filtering is by comparing a search term with the values of a column in a dataframe.

::: {.panel-tabset}
## Python

### The bracket notation
The first method is the classic syntax with the square brackets. Even though this method seems to be a bit from last century, it is the preferred method of many coders. 
```{python}
import pandas as pd

filtered_df = customers[customers.customer_name == 'Anthony Guerra']
filtered_df
```

### The `query` notation
Pandas offers a nice function that enables you to write queries in the following way:
```{python}
import pandas as pd

filtered_df = customers.query('customer_name == "Anthony Guerra"')
filtered_df
```
### The `loc` function
Provides the ability to filter dataframes by index names: df[row_name or filter,"column_name"].
```{python}
import pandas as pd

filtered_df = customers.loc[customers.customer_name == "Anthony Guerra",] # comma not necessary
filtered_df
```
### The `iloc` function
Provides the ability to filter dataframes by index: df[from:to,from:to].
```{python}
import pandas as pd

filtered_df = customers.iloc[20:25,2:4]
filtered_df
```

## R

### The bracket notation
The first method is the classic syntax with the square brackets. Even though this method seems to be a bit from last century, it is the preferred method of many coders. 
```{r}
filtered_df = customers[customers$customer_name == 'Anthony Guerra',] # comma is necessary!
filtered_df
```

### The dplyr method
```{r}
filtered_df <- customers %>% filter(customer_name == "Anthony Guerra")
filtered_df
```


## SQL

```{sql}
#| connection: con
select
  *
from 
  customers
where 0=0
  and customer_name = 'Anthony Guerra';
```
:::

## Filter by "is empty" 
Be honest, everyone hates empty values. However, we still have to deal with them!

::: {.panel-tabset}

## Python

### Drop rows where at least one column value is `NA`
```{python}
# add code here
import pandas as pd

df = pd.DataFrame({
    'A': [1, None, 3, None, None],
    'B': [5, None, None, None, None],
    'C': ['x', 'y', None, 'z', None]
})

filtered_df = df.dropna()
print(filtered_df)
```

### Drop rows where whole row is `None`
```{python}
import pandas as pd

df = pd.DataFrame({
    'A': [1, None, 3, None, None],
    'B': [5, None, None, None, None],
    'C': ['x', 'y', None, 'z', None]
})

filtered_df = df.dropna(how='all')
print(filtered_df)

```

## R

### Drop rows where at least one column value is `NA`
```{r}
library(dplyr)
library(tidyr)

df <- data.frame(
  A = c(1, NA, 3, NA, NA),
  B = c(5, NA, NA, NA, NA),
  C = c("x", "y", NA, "z", NA)
)

df %>% drop_na()
```

### Drop rows where whole row is `NA`
```{r}
library(dplyr)
library(tidyr)

df <- data.frame(
  A = c(1, NA, 3, NA, NA),
  B = c(5, NA, NA, NA, NA),
  C = c("x", "y", NA, "z", NA)
)

df %>% filter_all(any_vars(!is.na(.)))
```
## SQL

### Drop rows where at least one column value is `NULL`

```{sql}
#| eval: false
#| connection: con

select
  *
from
  schema.table
where 0=0
  and A is not null 
  and B is not null 
  and C is not null;

```

### Drop rows where all column values are `NULL`

```{sql}
#| eval: false
#| connection: con

select
  *
from
  schema.table
where 0=0
  coalesce(A, B, C) is not null;
```

:::

## Filter by random sample
Very useful to reduce large data

::: {.panel-tabset}

## Python
```{python}
import pandas as pd

reduced_df = orders.sample(frac=0.3, random_state=42)  # set a random seed for reproducibility

proportion = len(reduced_df) / len(orders) * 100
print(f"Proportion of random rows is: {proportion}%")

```

## R

```{r}
library(dplyr)

reduced_df <- orders %>% 
  slice_sample(prop = 0.3)

print(paste0("Proportion of random rows is: ", nrow(reduced_df)/nrow(orders)*100,"%" ))
```

## SQL

```{sql}
#| connection: con

select
  *
from
  orders
order by
  random()
limit (select count(*) * 0.3 from orders); -- 30% random sample


```


:::
