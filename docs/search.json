[
  {
    "objectID": "visualisations/goodpractices.html#stop-using-pie-charts",
    "href": "visualisations/goodpractices.html#stop-using-pie-charts",
    "title": "Good practices",
    "section": "Stop using pie charts!",
    "text": "Stop using pie charts!"
  },
  {
    "objectID": "visualisations/bar_chart.html",
    "href": "visualisations/bar_chart.html",
    "title": "Bar Chart",
    "section": "",
    "text": "R\n\n\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\nset.seed(123)\n\np &lt;- babynames %&gt;% \n  filter(year %in% c(1900, 1905, 1910), between(n, 1500, 8000)) %&gt;%\n  arrange(name, year) %&gt;% \n  slice_head(n = 30) %&gt;% \n  ggplot(\n    aes(\n      x = n,\n      y = fct_reorder(name,n, .fun=\"sum\"),\n      fill = factor(\n        year,\n        levels = c(1910, 1905, 1900)\n      )\n    )\n  ) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Bar Chart of Names by Year\",\n    x = \"Count\",\n    y = \"Name\",\n    fill = \"year\"\n  ) +\n  theme_minimal()\np"
  },
  {
    "objectID": "visualisations/bar_chart.html#perfect-for-1-categorial-1-numerical-variable",
    "href": "visualisations/bar_chart.html#perfect-for-1-categorial-1-numerical-variable",
    "title": "Bar Chart",
    "section": "",
    "text": "R\n\n\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\nset.seed(123)\n\np &lt;- babynames %&gt;% \n  filter(year %in% c(1900, 1905, 1910), between(n, 1500, 8000)) %&gt;%\n  arrange(name, year) %&gt;% \n  slice_head(n = 30) %&gt;% \n  ggplot(\n    aes(\n      x = n,\n      y = fct_reorder(name,n, .fun=\"sum\"),\n      fill = factor(\n        year,\n        levels = c(1910, 1905, 1900)\n      )\n    )\n  ) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Bar Chart of Names by Year\",\n    x = \"Count\",\n    y = \"Name\",\n    fill = \"year\"\n  ) +\n  theme_minimal()\np"
  },
  {
    "objectID": "visualisations/lineplot.html",
    "href": "visualisations/lineplot.html",
    "title": "Line plot",
    "section": "",
    "text": "RPython\n\n\n\n#library(ggplot2)\nlineplot &lt;- babynames %&gt;% \n  filter(name %in% c(\"Mona\", \"Anna\", \"Lisa\"), sex == \"F\") %&gt;% \n  ggplot(\n    aes(\n      x = year,\n      y = n,\n      color = name\n    )\n  ) +\n  geom_line() +\n  labs(\n    title = \"Amount of named Babies per year in US\",\n    subtitle = \"Selection of Anna, Mona and Lisa\",\n    x = \"Year\",\n    y = \"Number of Babies\",\n    color = \"Name\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"Anna\" = \"#d94a62\", \"Lisa\" = \"#a4eb8f\", \"Mona\"=\"darkblue\"),\n    breaks = c(\"Mona\", \"Lisa\",\"Anna\") # Order in the Legend\n  ) +\n  theme_minimal()\n\nprint(lineplot)\n\n\n\n\n\n\n\n\n# content coming soon"
  },
  {
    "objectID": "visualisations/lineplot.html#perfect-for-2-categorial-1-numerical-variables",
    "href": "visualisations/lineplot.html#perfect-for-2-categorial-1-numerical-variables",
    "title": "Line plot",
    "section": "",
    "text": "RPython\n\n\n\n#library(ggplot2)\nlineplot &lt;- babynames %&gt;% \n  filter(name %in% c(\"Mona\", \"Anna\", \"Lisa\"), sex == \"F\") %&gt;% \n  ggplot(\n    aes(\n      x = year,\n      y = n,\n      color = name\n    )\n  ) +\n  geom_line() +\n  labs(\n    title = \"Amount of named Babies per year in US\",\n    subtitle = \"Selection of Anna, Mona and Lisa\",\n    x = \"Year\",\n    y = \"Number of Babies\",\n    color = \"Name\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"Anna\" = \"#d94a62\", \"Lisa\" = \"#a4eb8f\", \"Mona\"=\"darkblue\"),\n    breaks = c(\"Mona\", \"Lisa\",\"Anna\") # Order in the Legend\n  ) +\n  theme_minimal()\n\nprint(lineplot)\n\n\n\n\n\n\n\n\n# content coming soon"
  },
  {
    "objectID": "visualisations/grammar_of_graphics.html",
    "href": "visualisations/grammar_of_graphics.html",
    "title": "Grammar of graphics",
    "section": "",
    "text": "There are plenty of data visualisation tools out there: BI tools such as Power BI, Tableau, MicroStrategy and so on are often used in larger companies who have a dedicated BI team. Many companies and universities however rely on open source data visualisation software and programming languages such as R (E.g. ggplot2, shiny) or Python (matplotlib, seaborn etc.). Depending on which technology is available and suitable for the specific use-case, the tool must be wisely chosen.\nThe grammar of graphics for the different tools can be fairly different from each other, this is why this section is dedicated to explaining the basic concepts of the different tools.\n\nR ggplot2\n\n\nThe grammar of graphics consists of the following components:\n\n\nFor example: ggplot(data=df, mapping=aes(x=col1, y=col2)). Aesthestics are things like the axes, shapes, colors, groups and so on. It is important to remember, that the aesthetics that are written inside the ggplot()will be inherited by the geometric objects, if they are not overwritten manually.\n\n\n\nThese are the different ways on how to display the data: E.g. by points (gemo_point()), lines (geom_line()), bars (geom_bar()) etc.\n\n\n\nScales define how the data is mapped to each aesthetic. E.g. scale_x_continous(), which is the default, or scale_y_log10() for lagarithmic scales.\n\n\n\nThe default coordinate system is the coord_cartesian(), whereas e.g. coord_polar() is used for circular visualisations such as the widely mobbed piechart.\n\n\n\nFacets are useful to split data into groups and visualise each group individually in smaller multiples.\n\n\n\nThemes give our plots a nice look and can be adjusted according to the style wanted."
  },
  {
    "objectID": "visualisations/grammar_of_graphics.html#syntax-depends-on-the-used-tool",
    "href": "visualisations/grammar_of_graphics.html#syntax-depends-on-the-used-tool",
    "title": "Grammar of graphics",
    "section": "",
    "text": "There are plenty of data visualisation tools out there: BI tools such as Power BI, Tableau, MicroStrategy and so on are often used in larger companies who have a dedicated BI team. Many companies and universities however rely on open source data visualisation software and programming languages such as R (E.g. ggplot2, shiny) or Python (matplotlib, seaborn etc.). Depending on which technology is available and suitable for the specific use-case, the tool must be wisely chosen.\nThe grammar of graphics for the different tools can be fairly different from each other, this is why this section is dedicated to explaining the basic concepts of the different tools.\n\nR ggplot2\n\n\nThe grammar of graphics consists of the following components:\n\n\nFor example: ggplot(data=df, mapping=aes(x=col1, y=col2)). Aesthestics are things like the axes, shapes, colors, groups and so on. It is important to remember, that the aesthetics that are written inside the ggplot()will be inherited by the geometric objects, if they are not overwritten manually.\n\n\n\nThese are the different ways on how to display the data: E.g. by points (gemo_point()), lines (geom_line()), bars (geom_bar()) etc.\n\n\n\nScales define how the data is mapped to each aesthetic. E.g. scale_x_continous(), which is the default, or scale_y_log10() for lagarithmic scales.\n\n\n\nThe default coordinate system is the coord_cartesian(), whereas e.g. coord_polar() is used for circular visualisations such as the widely mobbed piechart.\n\n\n\nFacets are useful to split data into groups and visualise each group individually in smaller multiples.\n\n\n\nThemes give our plots a nice look and can be adjusted according to the style wanted."
  },
  {
    "objectID": "visualisations/scatter.html",
    "href": "visualisations/scatter.html",
    "title": "Scatter plots",
    "section": "",
    "text": "RPython\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  facet_wrap(vars(Diet)) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n# For the grey dots:\n# A new dataset must be created where the column in the facet_wrap is missing\nChickWeight_wo_diet &lt;- ChickWeight %&gt;% select(-Diet)\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point(\n    data = ChickWeight_wo_diet,\n    mapping = aes(\n      x = Time,\n      y = weight),\n      color = \"grey\" # Overwriting the inherited color -&gt; adding grey dots\n  ) +\n  geom_point() + # adding the colored dots\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  facet_wrap(vars(Diet)) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  geom_smooth() + # for the fitted lines\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nmpg %&gt;%\n  ggplot(\n    aes(\n      x = displ,\n      y = hwy\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(\n    title = \"HWY vs displ in the mpg dataset\",\n    subtitle = \"With additional linear regression\",\n    x = \"Displacement\",\n    y = \"HWY\"\n  )\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- babynames %&gt;% \n  filter(name %in% c(\"Mona\", \"Anna\", \"Lisa\"), sex == \"F\") %&gt;% \n  ggplot(\n    aes(\n      x = year,\n      y = n,\n      color = name\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Amount of named Babies per year in US\",\n    subtitle = \"Selection of Anna, Mona and Lisa\",\n    x = \"Year\",\n    y = \"Number of Babies\",\n    color = \"Name\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"Anna\" = \"#d94a62\", \"Lisa\" = \"#a4eb8f\", \"Mona\"=\"darkblue\"),\n    breaks = c(\"Mona\", \"Lisa\",\"Anna\") # Order in the Legend\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n#library(ggplot2)\nhumans = starwars %&gt;% \n  filter(species == \"Human\", !is.na(mass)) %&gt;% \n  mutate(\n    BMI = mass/(height/100)^2\n  )\n\nscatter &lt;- humans %&gt;%\n  ggplot(\n    aes(\n      x=mass,\n      y=height,\n      color = BMI\n    )\n  ) +\n  geom_point(size = 4, alpha = 0.8) +\n  labs(\n    title = \"Length by Weight chart of famous Star Wars Heros\",\n    subtitle = \"Colored by BMI\",\n    x = \"Mass [kg]\",\n    y = \"Height [cm]\",\n    color = \"BMI\"  #Legend title\n  ) +\n  scale_color_gradient(\n    low = \"yellow\",\n    high = \"red\"\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n\n#library(ggplot2)\nhumans = starwars %&gt;% \n  filter(species == \"Human\", !is.na(mass)) %&gt;% \n  mutate(\n    BMI = mass/(height/100)^2\n  )\n\nscatter &lt;- humans %&gt;%\n  ggplot(\n    aes(\n      x=mass,\n      y=height,\n      color = BMI\n    )\n  ) +\n  geom_point(size = 4, alpha = 0.8) +\n  labs(\n    title = \"Length by Weight chart of famous Star Wars Heros\",\n    subtitle = \"Colored by BMI\",\n    x = \"Mass [kg]\",\n    y = \"Height [cm]\",\n    color = \"BMI\"  #Legend title\n  ) +\n  scale_color_gradient2(\n    low = \"yellow\",\n    mid = \"pink\",\n    midpoint = 27,\n    high = \"red\"\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nscatter = sns.scatterplot(\n    data=ChickWeight,\n    x='Time',\n    y='weight',\n    hue='Diet',\n    palette={\n        \"1\": \"#d94a62\",\n        \"2\": \"#6d61e8\", \n        \"3\": \"#e3d97f\",\n        \"4\": \"#a4eb8f\"\n    }\n)\n\nplt.title(\"Weight by Chicken over time\")\nplt.xlabel(\"Time [days]\")\nplt.ylabel(\"Weight [g]\")\nplt.legend(title=\"Diet\")\nplt.show()"
  },
  {
    "objectID": "visualisations/scatter.html#point-plots-for-2d-data-both-dimensions-numerical",
    "href": "visualisations/scatter.html#point-plots-for-2d-data-both-dimensions-numerical",
    "title": "Scatter plots",
    "section": "",
    "text": "RPython\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  facet_wrap(vars(Diet)) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n# For the grey dots:\n# A new dataset must be created where the column in the facet_wrap is missing\nChickWeight_wo_diet &lt;- ChickWeight %&gt;% select(-Diet)\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point(\n    data = ChickWeight_wo_diet,\n    mapping = aes(\n      x = Time,\n      y = weight),\n      color = \"grey\" # Overwriting the inherited color -&gt; adding grey dots\n  ) +\n  geom_point() + # adding the colored dots\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  facet_wrap(vars(Diet)) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- ChickWeight %&gt;% \n  ggplot(\n    aes(\n      x = Time,\n      y = weight,\n      color = as.factor(Diet)\n    )\n  ) +\n  geom_point() +\n  geom_smooth() + # for the fitted lines\n  labs(\n    title = \"Weight by Chicken over time\",\n    subtitle = \"The chicken were fed with 4 different diets.\",\n    x = \"Time [days]\",\n    y = \"Weight [g]\",\n    color = \"Diet\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"1\" = \"#d94a62\", \"2\" = \"#6d61e8\", \"3\" = \"#e3d97f\", \"4\" = \"#a4eb8f\"),\n    breaks = c(\"1\", \"2\", \"3\", \"4\")\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n#library(ggplot2)\nmpg %&gt;%\n  ggplot(\n    aes(\n      x = displ,\n      y = hwy\n    )\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(\n    title = \"HWY vs displ in the mpg dataset\",\n    subtitle = \"With additional linear regression\",\n    x = \"Displacement\",\n    y = \"HWY\"\n  )\n\n\n\n\n\n#library(ggplot2)\nscatter &lt;- babynames %&gt;% \n  filter(name %in% c(\"Mona\", \"Anna\", \"Lisa\"), sex == \"F\") %&gt;% \n  ggplot(\n    aes(\n      x = year,\n      y = n,\n      color = name\n    )\n  ) +\n  geom_point() +\n  labs(\n    title = \"Amount of named Babies per year in US\",\n    subtitle = \"Selection of Anna, Mona and Lisa\",\n    x = \"Year\",\n    y = \"Number of Babies\",\n    color = \"Name\"  #Legend title\n  ) +\n  scale_color_manual(\n    values = c(\"Anna\" = \"#d94a62\", \"Lisa\" = \"#a4eb8f\", \"Mona\"=\"darkblue\"),\n    breaks = c(\"Mona\", \"Lisa\",\"Anna\") # Order in the Legend\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n#library(ggplot2)\nhumans = starwars %&gt;% \n  filter(species == \"Human\", !is.na(mass)) %&gt;% \n  mutate(\n    BMI = mass/(height/100)^2\n  )\n\nscatter &lt;- humans %&gt;%\n  ggplot(\n    aes(\n      x=mass,\n      y=height,\n      color = BMI\n    )\n  ) +\n  geom_point(size = 4, alpha = 0.8) +\n  labs(\n    title = \"Length by Weight chart of famous Star Wars Heros\",\n    subtitle = \"Colored by BMI\",\n    x = \"Mass [kg]\",\n    y = \"Height [cm]\",\n    color = \"BMI\"  #Legend title\n  ) +\n  scale_color_gradient(\n    low = \"yellow\",\n    high = \"red\"\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n\n#library(ggplot2)\nhumans = starwars %&gt;% \n  filter(species == \"Human\", !is.na(mass)) %&gt;% \n  mutate(\n    BMI = mass/(height/100)^2\n  )\n\nscatter &lt;- humans %&gt;%\n  ggplot(\n    aes(\n      x=mass,\n      y=height,\n      color = BMI\n    )\n  ) +\n  geom_point(size = 4, alpha = 0.8) +\n  labs(\n    title = \"Length by Weight chart of famous Star Wars Heros\",\n    subtitle = \"Colored by BMI\",\n    x = \"Mass [kg]\",\n    y = \"Height [cm]\",\n    color = \"BMI\"  #Legend title\n  ) +\n  scale_color_gradient2(\n    low = \"yellow\",\n    mid = \"pink\",\n    midpoint = 27,\n    high = \"red\"\n  ) +\n  theme_minimal()\n\nprint(scatter)\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nscatter = sns.scatterplot(\n    data=ChickWeight,\n    x='Time',\n    y='weight',\n    hue='Diet',\n    palette={\n        \"1\": \"#d94a62\",\n        \"2\": \"#6d61e8\", \n        \"3\": \"#e3d97f\",\n        \"4\": \"#a4eb8f\"\n    }\n)\n\nplt.title(\"Weight by Chicken over time\")\nplt.xlabel(\"Time [days]\")\nplt.ylabel(\"Weight [g]\")\nplt.legend(title=\"Diet\")\nplt.show()"
  },
  {
    "objectID": "visualisations/heatmap.html",
    "href": "visualisations/heatmap.html",
    "title": "Heat maps",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\n#library(ggplot2)\nbabynames %&gt;% \n  filter(\n    year %in% 2014:2017,\n    n &gt; 1000\n    ) %&gt;%\n  group_by(name, sex) %&gt;% \n  summarize(mean = mean(n)) %&gt;% \n  ungroup() %&gt;% \n  group_by(name) %&gt;% \n  mutate(\n    proz = mean/sum(mean)*100,\n    diff = abs(proz-50)\n  ) %&gt;% \n  ungroup() %&gt;%\n  arrange(diff) %&gt;% \n  slice_head(n=8) %&gt;% \n  ggplot(\n    aes(\n      x = name,\n      y = sex,\n      fill = proz\n    )\n  ) +\n  geom_tile() +\n  labs(\n    title = \"Babynames for both boyz and girls\",\n    subtitle = \"Powered by the babynames package\",\n    x = \"Name\",\n    y = \"Gender\",\n    fill = \"%\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n# content coming soon"
  },
  {
    "objectID": "visualisations/heatmap.html#perfect-for-2-categorial-1-numerical-variables",
    "href": "visualisations/heatmap.html#perfect-for-2-categorial-1-numerical-variables",
    "title": "Heat maps",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\n#library(ggplot2)\nbabynames %&gt;% \n  filter(\n    year %in% 2014:2017,\n    n &gt; 1000\n    ) %&gt;%\n  group_by(name, sex) %&gt;% \n  summarize(mean = mean(n)) %&gt;% \n  ungroup() %&gt;% \n  group_by(name) %&gt;% \n  mutate(\n    proz = mean/sum(mean)*100,\n    diff = abs(proz-50)\n  ) %&gt;% \n  ungroup() %&gt;%\n  arrange(diff) %&gt;% \n  slice_head(n=8) %&gt;% \n  ggplot(\n    aes(\n      x = name,\n      y = sex,\n      fill = proz\n    )\n  ) +\n  geom_tile() +\n  labs(\n    title = \"Babynames for both boyz and girls\",\n    subtitle = \"Powered by the babynames package\",\n    x = \"Name\",\n    y = \"Gender\",\n    fill = \"%\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n# content coming soon"
  },
  {
    "objectID": "visualisations/boxplot.html",
    "href": "visualisations/boxplot.html",
    "title": "Boxplots",
    "section": "",
    "text": "R\n\n\n\n#library(ggplot2)\nboxplot &lt;- mpg %&gt;% \n  ggplot(\n    aes(\n      x = class,\n      y = hwy,\n      fill = class\n    )\n  ) +\n  geom_boxplot() +\n  geom_jitter(alpha=0.3)+\n  labs(\n    title = \"Distribution of hwy per class\",\n    #subtitle = \"\",\n    x = \"Car class\",\n    y = \"hwy\",\n    #color = \"\"  #Legend title\n  ) +\n  theme_minimal()\n\nprint(boxplot)\n\n\n\n\n\n#library(ggplot2)\nlibrary(forcats)\nboxplot &lt;- mpg %&gt;% \n  ggplot(\n    aes(\n      x = hwy,\n      # reorder class-factors by median of hwy descending\n      y = fct_reorder(class, hwy, .fun=median, .desc=T),\n      fill = class \n    )\n  ) +\n  geom_boxplot() +\n  geom_jitter(alpha=0.3) +\n  labs(\n    title = \"Distribution of hwy per class\",\n    x = \"hwy\",\n    y = \"Car class\",\n    caption = \"Pickups are the coolest\"\n  ) +\n  theme_minimal()\n\nprint(boxplot)"
  },
  {
    "objectID": "visualisations/boxplot.html#visualizing-distributions-with-boxplots",
    "href": "visualisations/boxplot.html#visualizing-distributions-with-boxplots",
    "title": "Boxplots",
    "section": "",
    "text": "R\n\n\n\n#library(ggplot2)\nboxplot &lt;- mpg %&gt;% \n  ggplot(\n    aes(\n      x = class,\n      y = hwy,\n      fill = class\n    )\n  ) +\n  geom_boxplot() +\n  geom_jitter(alpha=0.3)+\n  labs(\n    title = \"Distribution of hwy per class\",\n    #subtitle = \"\",\n    x = \"Car class\",\n    y = \"hwy\",\n    #color = \"\"  #Legend title\n  ) +\n  theme_minimal()\n\nprint(boxplot)\n\n\n\n\n\n#library(ggplot2)\nlibrary(forcats)\nboxplot &lt;- mpg %&gt;% \n  ggplot(\n    aes(\n      x = hwy,\n      # reorder class-factors by median of hwy descending\n      y = fct_reorder(class, hwy, .fun=median, .desc=T),\n      fill = class \n    )\n  ) +\n  geom_boxplot() +\n  geom_jitter(alpha=0.3) +\n  labs(\n    title = \"Distribution of hwy per class\",\n    x = \"hwy\",\n    y = \"Car class\",\n    caption = \"Pickups are the coolest\"\n  ) +\n  theme_minimal()\n\nprint(boxplot)"
  },
  {
    "objectID": "coding-styles/functions.html",
    "href": "coding-styles/functions.html",
    "title": "Functional",
    "section": "",
    "text": "Following the DRY principle (don’t repeat yourself), functional programming allows the coder to reuse certain chunks of code. The input values are called arguments, while to actual logic performed by the function happens in the body. Functions also create a local environment.\n\n\n\nR\n\n\n\n\nThere are 4 types of functions in R.\n\n\n\n\na = \"start_a\"\nb = \"start_b\"\n\ndo_sth &lt;- function() {\n  a = 1\n  b = 2\n  print(paste(a,b))\n}\n\ndo_sth_else &lt;- function() {\n  a = 1\n  print(paste(a,b))\n}\n\ndo_sth()\n\n[1] \"1 2\"\n\ndo_sth_else()\n\n[1] \"1 start_b\"\n\nprint(paste(a, b))\n\n[1] \"start_a start_b\"\n\n\n\n\n\nFunctions can be grouped by adding them to a list.\n\nfun &lt;- list(\n  dog = function() print(\"bark\"),\n  cat = function() print(\"meow\"),\n  fox = function() print(\"What does the fox say?\")\n)\n\nfun$fox()\n\n[1] \"What does the fox say?\"\n\n\n\n\n\n\n\nPythonR\n\n\n\nimport pandas as pd\n\ndef get_product_quantity(product):\n  \"\"\"\n  Function to get the total quantity of a specific product from orders table.\n  \n  Arguments:\n  product: A string specifying the product name.\n  \n  Returns:\n  The total quantity of the specified product.\n  \"\"\"\n  merged_data = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n  filtered_data = merged_data[merged_data['product_name'] == product]\n  quantity_sum = filtered_data['quantity'].sum()\n  return quantity_sum\n\nget_product_quantity(\"field\")\n\n46.0\n\n\n\n\n\nget_product_quantity &lt;- function(product) {\n  result &lt;- orders %&gt;%\n    inner_join(products, by = \"product_id\") %&gt;% \n    filter(product_name == product) %&gt;% \n    summarize(sum = sum(quantity)) %&gt;% \n    pull(sum) \n  return(result)\n}\n\nget_product_quantity(\"field\")\n\n[1] 46"
  },
  {
    "objectID": "coding-styles/functions.html#functional-programming",
    "href": "coding-styles/functions.html#functional-programming",
    "title": "Functional",
    "section": "",
    "text": "Following the DRY principle (don’t repeat yourself), functional programming allows the coder to reuse certain chunks of code. The input values are called arguments, while to actual logic performed by the function happens in the body. Functions also create a local environment.\n\n\n\nR\n\n\n\n\nThere are 4 types of functions in R.\n\n\n\n\na = \"start_a\"\nb = \"start_b\"\n\ndo_sth &lt;- function() {\n  a = 1\n  b = 2\n  print(paste(a,b))\n}\n\ndo_sth_else &lt;- function() {\n  a = 1\n  print(paste(a,b))\n}\n\ndo_sth()\n\n[1] \"1 2\"\n\ndo_sth_else()\n\n[1] \"1 start_b\"\n\nprint(paste(a, b))\n\n[1] \"start_a start_b\"\n\n\n\n\n\nFunctions can be grouped by adding them to a list.\n\nfun &lt;- list(\n  dog = function() print(\"bark\"),\n  cat = function() print(\"meow\"),\n  fox = function() print(\"What does the fox say?\")\n)\n\nfun$fox()\n\n[1] \"What does the fox say?\"\n\n\n\n\n\n\n\nPythonR\n\n\n\nimport pandas as pd\n\ndef get_product_quantity(product):\n  \"\"\"\n  Function to get the total quantity of a specific product from orders table.\n  \n  Arguments:\n  product: A string specifying the product name.\n  \n  Returns:\n  The total quantity of the specified product.\n  \"\"\"\n  merged_data = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n  filtered_data = merged_data[merged_data['product_name'] == product]\n  quantity_sum = filtered_data['quantity'].sum()\n  return quantity_sum\n\nget_product_quantity(\"field\")\n\n46.0\n\n\n\n\n\nget_product_quantity &lt;- function(product) {\n  result &lt;- orders %&gt;%\n    inner_join(products, by = \"product_id\") %&gt;% \n    filter(product_name == product) %&gt;% \n    summarize(sum = sum(quantity)) %&gt;% \n    pull(sum) \n  return(result)\n}\n\nget_product_quantity(\"field\")\n\n[1] 46"
  },
  {
    "objectID": "data-wrangling/exporting_data.html",
    "href": "data-wrangling/exporting_data.html",
    "title": "Exporting data",
    "section": "",
    "text": "Excel/CSVs are a very handy format to transport (relatively) small amounts of data between stakeholders.\n\nPythonR\n\n\n\n#add code here\n\n\n\n\nlibrary(openxlsx)\nwb &lt;- createWorkbook()\nsheet &lt;- addWorksheet(wb, sheetName = \"my_sheet_name\")\nwriteData(wb, sheet, df, startCol = 1, startRow = 1)\nsaveWorkbook(wb,\n             file = \"path.xlsx\",\n             overwrite = TRUE)"
  },
  {
    "objectID": "data-wrangling/exporting_data.html#excel-files-and-csvs",
    "href": "data-wrangling/exporting_data.html#excel-files-and-csvs",
    "title": "Exporting data",
    "section": "",
    "text": "Excel/CSVs are a very handy format to transport (relatively) small amounts of data between stakeholders.\n\nPythonR\n\n\n\n#add code here\n\n\n\n\nlibrary(openxlsx)\nwb &lt;- createWorkbook()\nsheet &lt;- addWorksheet(wb, sheetName = \"my_sheet_name\")\nwriteData(wb, sheet, df, startCol = 1, startRow = 1)\nsaveWorkbook(wb,\n             file = \"path.xlsx\",\n             overwrite = TRUE)"
  },
  {
    "objectID": "data-wrangling/reading_data.html",
    "href": "data-wrangling/reading_data.html",
    "title": "Importing data",
    "section": "",
    "text": "Reading in data from various sources is one of the key skills to get a day of data wrangling started. There are tons of vanilla functions as well as cool packages to do the job."
  },
  {
    "objectID": "data-wrangling/reading_data.html#excel-files-and-csvs-tsvs",
    "href": "data-wrangling/reading_data.html#excel-files-and-csvs-tsvs",
    "title": "Importing data",
    "section": "Excel files and CSVs / TSVs",
    "text": "Excel files and CSVs / TSVs\nExcel/CSVs are a very handy format to transport (relatively) small amounts of data between stakeholders.\n\nPythonR\n\n\n\nimport pandas as pd\ndf = pd.read_excel(\"path.xlsx\", sheet_name=\"Sheet1\") # for Excels\ndf = pd.read_csv(\"path.csv\") # for comma separated\ndf = pd.read_csv(\"path.csv\", sep=';') # for semicolon separated\n\n\n\n\n#library(readr)\ndf &lt;- read_excel(\"path\", sheet = \"Sheet1\", range = \"B4:F27\") # readxl package\ndf &lt;- read.csv(\"path\")  # comma separated\ndf &lt;- read.csv2(\"path\") # semicolon separated\ndf &lt;- read_csv(\"path\")  # readr package\ndf &lt;- read_csv2(\"path\")  # readr package for semicolon, and comma as decimal point\ndf &lt;- read_tsv(\"path\")  # readr package for tab separated files"
  },
  {
    "objectID": "data-wrangling/reading_data.html#connecting-to-databases",
    "href": "data-wrangling/reading_data.html#connecting-to-databases",
    "title": "Importing data",
    "section": "Connecting to databases",
    "text": "Connecting to databases\nThe most convenient way of analysing data is by simply connecting to a DWH or DB directly. This can be achieved using pre-built packages.\n\nPythonR\n\n\n\nimport sqlite3\nimport pandas as pd\n\ncon = sqlite3.connect(\"path_to_db.sqlite\")\n\n# Get a list of all tables in the database\ncursor = con.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = cursor.fetchall()\nprint(\"Tables in the database:\", tables)\n\ncursor.execute(\"SELECT * FROM orders;\")\nrows = cursor.fetchall()\n\nrows.head()\n\n# Close the connection\ncon.close()\n\n\n\n\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"path_to_db.sqlite\")\n\ndb_list_tables(con) # show all tables in the db\n\norders &lt;- tbl(con, \"orders\")\nhead(orders)\n\ndbDisconnect(shop_db)"
  },
  {
    "objectID": "data-wrangling/loops.html",
    "href": "data-wrangling/loops.html",
    "title": "Loops",
    "section": "",
    "text": "for loops are probably the most used form of loops. A for loop iterates over iterable objects such as lists, vectors, sets and other data structures. In contrast to the while loop, the for loop is less prone to ending up in infinite loop and is therefore preferable. :::{{.callout-note}} Note, that for loops can often be replaced with the more sophisticated apply, sapply or lapply. :::\n\nPythonR\n\n\nIterate over the items in a list:\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i in x:\n    print(i)\n\nhello\nmy\nname\nis\neugen\nwhat's\nyour\nname?\n\n\nGet the index of the element using enumerate():\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i, value in enumerate(x):\n    print(f\"{i} {value}\")\n\n0 hello\n1 my\n2 name\n3 is\n4 eugen\n5 what's\n6 your\n7 name?\n\n\n\n\n\n\n\n\nCaution\n\n\n\nKeep in mind, that iterables start with the index 0 in python\n\n\nJump to next iteration using continue, exit the loop using break:\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i in x:\n    if i in [\"my\", \"name\", \"is\"]:\n        continue\n    else:\n        print(i)\n        if i == \"eugen\":\n            break\n\nhello\neugen\n\n\n\n\nIterate over the items in a vector:\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in x) {\n  print(i)\n}\n\n[1] \"hello\"\n[1] \"my\"\n[1] \"name\"\n[1] \"is\"\n[1] \"eugen\"\n[1] \"what's\"\n[1] \"your\"\n[1] \"name?\"\n\n\nGet the index of the element using seq_along():\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in seq_along(x)) { # always use seq_along instead of 1:length(x) or 1:nrow(df)\n  print(paste(i,x[i]))\n}\n\n[1] \"1 hello\"\n[1] \"2 my\"\n[1] \"3 name\"\n[1] \"4 is\"\n[1] \"5 eugen\"\n[1] \"6 what's\"\n[1] \"7 your\"\n[1] \"8 name?\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nKeep in mind, that iterables start with the index 1 in R.\n\n\nJump to next iteration using next, exit the loop using break:\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in x) {\n  if (i %in% c(\"my\", \"name\", \"is\")) {\n    next\n  } else {\n    print(i)\n    if (i == \"eugen\") break\n  }\n}\n\n[1] \"hello\"\n[1] \"eugen\""
  },
  {
    "objectID": "data-wrangling/loops.html#the-for-loop",
    "href": "data-wrangling/loops.html#the-for-loop",
    "title": "Loops",
    "section": "",
    "text": "for loops are probably the most used form of loops. A for loop iterates over iterable objects such as lists, vectors, sets and other data structures. In contrast to the while loop, the for loop is less prone to ending up in infinite loop and is therefore preferable. :::{{.callout-note}} Note, that for loops can often be replaced with the more sophisticated apply, sapply or lapply. :::\n\nPythonR\n\n\nIterate over the items in a list:\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i in x:\n    print(i)\n\nhello\nmy\nname\nis\neugen\nwhat's\nyour\nname?\n\n\nGet the index of the element using enumerate():\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i, value in enumerate(x):\n    print(f\"{i} {value}\")\n\n0 hello\n1 my\n2 name\n3 is\n4 eugen\n5 what's\n6 your\n7 name?\n\n\n\n\n\n\n\n\nCaution\n\n\n\nKeep in mind, that iterables start with the index 0 in python\n\n\nJump to next iteration using continue, exit the loop using break:\n\nx = [\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\"]\nfor i in x:\n    if i in [\"my\", \"name\", \"is\"]:\n        continue\n    else:\n        print(i)\n        if i == \"eugen\":\n            break\n\nhello\neugen\n\n\n\n\nIterate over the items in a vector:\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in x) {\n  print(i)\n}\n\n[1] \"hello\"\n[1] \"my\"\n[1] \"name\"\n[1] \"is\"\n[1] \"eugen\"\n[1] \"what's\"\n[1] \"your\"\n[1] \"name?\"\n\n\nGet the index of the element using seq_along():\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in seq_along(x)) { # always use seq_along instead of 1:length(x) or 1:nrow(df)\n  print(paste(i,x[i]))\n}\n\n[1] \"1 hello\"\n[1] \"2 my\"\n[1] \"3 name\"\n[1] \"4 is\"\n[1] \"5 eugen\"\n[1] \"6 what's\"\n[1] \"7 your\"\n[1] \"8 name?\"\n\n\n\n\n\n\n\n\nCaution\n\n\n\nKeep in mind, that iterables start with the index 1 in R.\n\n\nJump to next iteration using next, exit the loop using break:\n\nx = c(\"hello\", \"my\", \"name\", \"is\", \"eugen\", \"what's\", \"your\", \"name?\")\nfor (i in x) {\n  if (i %in% c(\"my\", \"name\", \"is\")) {\n    next\n  } else {\n    print(i)\n    if (i == \"eugen\") break\n  }\n}\n\n[1] \"hello\"\n[1] \"eugen\""
  },
  {
    "objectID": "data-wrangling/filtering.html",
    "href": "data-wrangling/filtering.html",
    "title": "Filtering",
    "section": "",
    "text": "Filtering dataframes is a crucial skill of any data scientist and there is probably the most used method in data wrangling."
  },
  {
    "objectID": "data-wrangling/filtering.html#basic-filtering-techniques-.-etc.",
    "href": "data-wrangling/filtering.html#basic-filtering-techniques-.-etc.",
    "title": "Filtering",
    "section": "Basic filtering techniques (==, <. != etc.)",
    "text": "Basic filtering techniques (==, &lt;. != etc.)\nThe most basic form of filtering is by comparing a search term with the values of a column in a dataframe.\n\nPythonRSQL\n\n\n\nThe bracket notation\nThe first method is the classic syntax with the square brackets. Even though this method seems to be a bit from last century, it is the preferred method of many coders.\n\nimport pandas as pd\n\nfiltered_df = customers[customers.customer_name == 'Anthony Guerra']\nfiltered_df\n\n                            customer_id  ...                                            address\n0  8ab2552b-e4c9-471d-bdf1-ff6e225860e1  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n\n[1 rows x 5 columns]\n\n\n\n\nThe query notation\nPandas offers a nice function that enables you to write queries in the following way:\n\nimport pandas as pd\n\nfiltered_df = customers.query('customer_name == \"Anthony Guerra\"')\nfiltered_df\n\n                            customer_id  ...                                            address\n0  8ab2552b-e4c9-471d-bdf1-ff6e225860e1  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n\n[1 rows x 5 columns]\n\n\n\n\nThe loc function\nProvides the ability to filter dataframes by index names: df[row_name or filter,“column_name”].\n\nimport pandas as pd\n\nfiltered_df = customers.loc[customers.customer_name == \"Anthony Guerra\",] # comma not necessary\nfiltered_df\n\n                            customer_id  ...                                            address\n0  8ab2552b-e4c9-471d-bdf1-ff6e225860e1  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n\n[1 rows x 5 columns]\n\n\n\n\nThe iloc function\nProvides the ability to filter dataframes by index: df[from:to,from:to].\n\nimport pandas as pd\n\nfiltered_df = customers.iloc[20:25,2:4]\nfiltered_df\n\n                        email         phone_number\n20      jeffrey71@example.com     001-582-898-9180\n21       jandrews@example.com  (652)154-8443x09491\n22        robin69@example.com           9724123889\n23         mary10@example.com           8598893292\n24  gonzaleznorma@example.org        (278)903-9109\n\n\n\n\n\n\nThe bracket notation\nThe first method is the classic syntax with the square brackets. Even though this method seems to be a bit from last century, it is the preferred method of many coders.\n\nfiltered_df = customers[customers$customer_name == 'Anthony Guerra',] # comma is necessary!\nfiltered_df\n\n# A tibble: 1 × 5\n  customer_id                          customer_name  email phone_number address\n  &lt;chr&gt;                                &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;  \n1 8ab2552b-e4c9-471d-bdf1-ff6e225860e1 Anthony Guerra jane… 659.461.7580 \"963 R…\n\n\n\n\nThe dplyr method\n\nfiltered_df &lt;- customers %&gt;% filter(customer_name == \"Anthony Guerra\")\nfiltered_df\n\n# A tibble: 1 × 5\n  customer_id                          customer_name  email phone_number address\n  &lt;chr&gt;                                &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;  \n1 8ab2552b-e4c9-471d-bdf1-ff6e225860e1 Anthony Guerra jane… 659.461.7580 \"963 R…\n\n\n\n\n\n\nselect\n  *\nfrom \n  customers\nwhere 0=0\n  and customer_name = 'Anthony Guerra';\n\n\n1 records\n\n\n\n\n\n\n\n\n\ncustomer_id\ncustomer_name\nemail\nphone_number\naddress\n\n\n\n\n8ab2552b-e4c9-471d-bdf1-ff6e225860e1\nAnthony Guerra\njanet90@example.net\n659.461.7580\n963 Regina Bridge Apt. 279\n\n\nPort Richard, NJ 42041"
  },
  {
    "objectID": "data-wrangling/filtering.html#filter-by-is-empty",
    "href": "data-wrangling/filtering.html#filter-by-is-empty",
    "title": "Filtering",
    "section": "Filter by “is empty”",
    "text": "Filter by “is empty”\nBe honest, everyone hates empty values. However, we still have to deal with them!\n\nPythonRSQL\n\n\n\nDrop rows where at least one column value is NA\n\n# add code here\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, None, 3, None, None],\n    'B': [5, None, None, None, None],\n    'C': ['x', 'y', None, 'z', None]\n})\n\nfiltered_df = df.dropna()\nprint(filtered_df)\n\n     A    B  C\n0  1.0  5.0  x\n\n\n\n\nDrop rows where whole row is None\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, None, 3, None, None],\n    'B': [5, None, None, None, None],\n    'C': ['x', 'y', None, 'z', None]\n})\n\nfiltered_df = df.dropna(how='all')\nprint(filtered_df)\n\n     A    B     C\n0  1.0  5.0     x\n1  NaN  NaN     y\n2  3.0  NaN  None\n3  NaN  NaN     z\n\n\n\n\n\n\nDrop rows where at least one column value is NA\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf &lt;- data.frame(\n  A = c(1, NA, 3, NA, NA),\n  B = c(5, NA, NA, NA, NA),\n  C = c(\"x\", \"y\", NA, \"z\", NA)\n)\n\ndf %&gt;% drop_na()\n\n  A B C\n1 1 5 x\n\n\n\n\nDrop rows where whole row is NA\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf &lt;- data.frame(\n  A = c(1, NA, 3, NA, NA),\n  B = c(5, NA, NA, NA, NA),\n  C = c(\"x\", \"y\", NA, \"z\", NA)\n)\n\ndf %&gt;% filter_all(any_vars(!is.na(.)))\n\n   A  B    C\n1  1  5    x\n2 NA NA    y\n3  3 NA &lt;NA&gt;\n4 NA NA    z\n\n\n\n\n\n\nDrop rows where at least one column value is NULL\n\nselect\n  *\nfrom\n  schema.table\nwhere 0=0\n  and A is not null \n  and B is not null \n  and C is not null;\n\n\n\nDrop rows where all column values are NULL\n\nselect\n  *\nfrom\n  schema.table\nwhere 0=0\n  coalesce(A, B, C) is not null;"
  },
  {
    "objectID": "data-wrangling/filtering.html#filter-by-random-sample",
    "href": "data-wrangling/filtering.html#filter-by-random-sample",
    "title": "Filtering",
    "section": "Filter by random sample",
    "text": "Filter by random sample\nVery useful to reduce large data\n\nPythonRSQL\n\n\n\nimport pandas as pd\n\nreduced_df = orders.sample(frac=0.3, random_state=42)  # set a random seed for reproducibility\n\nproportion = len(reduced_df) / len(orders) * 100\nprint(f\"Proportion of random rows is: {proportion}%\")\n\nProportion of random rows is: 30.0%\n\n\n\n\n\nlibrary(dplyr)\n\nreduced_df &lt;- orders %&gt;% \n  slice_sample(prop = 0.3)\n\nprint(paste0(\"Proportion of random rows is: \", nrow(reduced_df)/nrow(orders)*100,\"%\" ))\n\n[1] \"Proportion of random rows is: 30%\"\n\n\n\n\n\nselect\n  *\nfrom\n  orders\norder by\n  random()\nlimit (select count(*) * 0.3 from orders); -- 30% random sample\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\nproduct_id\nquantity\norder_date\ndelivery_date\n\n\n\n\nad1e3c1e-1c7b-4754-9dfb-74927e40f0e9\n5af19bbf-5172-4024-a7bf-00bd2839f125\n5\n5\n2023-09-22\n2023-12-01\n\n\n1f6ad345-3246-418d-8b64-ca63e1159a60\n647fdcac-fbb1-44f5-b4e7-026e947ba187\n27\n3\n2023-02-01\n2023-12-10\n\n\nccff8ee8-7b91-42c9-bad8-f396d2c4ad85\na6617a27-91a8-4fc2-9456-53c391f33078\n7\n5\n2023-06-24\n2023-12-20\n\n\n62e29f35-aa89-4a13-bba6-b4d898663965\n3a28df17-0bce-4c97-a80c-98ce4eed015a\n1\n4\n2023-04-15\n2023-12-18\n\n\nd9eaff96-b685-4d7e-9882-863249da9b45\nb9600234-591a-4a06-8add-4bb561fb835c\n9\n5\n2023-02-11\n2023-11-26\n\n\naa11d42b-f2fa-45f3-8241-9d7a9c6139d7\naca6a334-cf39-470c-bdee-774a0458de4a\n14\n3\n2023-08-22\n2023-12-25\n\n\n4ad2cd0a-9e63-4467-8a4f-ca13f7f6249e\ncf175a11-1d2e-4eed-ba0b-daf970d8314e\n24\n1\n2023-07-09\n2023-11-26\n\n\nf835d8d9-d8ee-4eb9-8da0-3328fe5cdd7f\nbbf02c38-13b8-4181-9f00-8bdd4bd7ae1d\n23\n3\n2023-05-24\n2023-12-12\n\n\n054da501-e596-49ec-a4d7-225bf2d01724\n086af576-2f26-453c-ac15-5a597d7b9894\n10\n4\n2023-08-09\n2023-12-18\n\n\n44818a58-f1a6-41c0-afa0-45bac19dff33\n95af9ac8-1ca8-45e9-a272-465e118b79e4\n10\n4\n2023-05-24\n2023-12-10"
  },
  {
    "objectID": "data-wrangling/dates.html",
    "href": "data-wrangling/dates.html",
    "title": "Dates",
    "section": "",
    "text": "PythonRSQL\n\n\n\nfrom datetime import datetime\ndate = datetime.strptime(\"120506\", \"%y%m%d\").date()\nprint(date)\n\n2012-05-06\n\n\n\n\n\n\n\nlibrary(lubridate)\n\n# year-month-day, output will be of class date\nymd(c(\"070605\", \"07-06-05\", \"07.06.05\", \"2007 06 05\"))\n\n[1] \"2007-06-05\" \"2007-06-05\" \"2007-06-05\" \"2007-06-05\"\n\n# year-day-month output will be of class date\nydm(c(\"070605\", \"07-06-05\", \"07.06.05\", \"2007 06 05\")) \n\n[1] \"2007-05-06\" \"2007-05-06\" \"2007-05-06\" \"2007-05-06\"\n\n\n\n\n\n\n# Make sure to check encodings / locale\nparse_date_time(\"7 Apr 23\", orders = \"dby\") \n\n[1] \"2023-04-07 UTC\"\n\n\n\n\n\n\nSELECT\n    DATE('20' || SUBSTR('070605', 1, 2) || '-' || SUBSTR('070605', 3, 2) || '-' || SUBSTR('070605', 5, 2)) AS parsed_date;\n\n\n1 records\n\n\nparsed_date\n\n\n\n\n2007-06-05"
  },
  {
    "objectID": "data-wrangling/dates.html#converting-strings-to-dates",
    "href": "data-wrangling/dates.html#converting-strings-to-dates",
    "title": "Dates",
    "section": "",
    "text": "PythonRSQL\n\n\n\nfrom datetime import datetime\ndate = datetime.strptime(\"120506\", \"%y%m%d\").date()\nprint(date)\n\n2012-05-06\n\n\n\n\n\n\n\nlibrary(lubridate)\n\n# year-month-day, output will be of class date\nymd(c(\"070605\", \"07-06-05\", \"07.06.05\", \"2007 06 05\"))\n\n[1] \"2007-06-05\" \"2007-06-05\" \"2007-06-05\" \"2007-06-05\"\n\n# year-day-month output will be of class date\nydm(c(\"070605\", \"07-06-05\", \"07.06.05\", \"2007 06 05\")) \n\n[1] \"2007-05-06\" \"2007-05-06\" \"2007-05-06\" \"2007-05-06\"\n\n\n\n\n\n\n# Make sure to check encodings / locale\nparse_date_time(\"7 Apr 23\", orders = \"dby\") \n\n[1] \"2023-04-07 UTC\"\n\n\n\n\n\n\nSELECT\n    DATE('20' || SUBSTR('070605', 1, 2) || '-' || SUBSTR('070605', 3, 2) || '-' || SUBSTR('070605', 5, 2)) AS parsed_date;\n\n\n1 records\n\n\nparsed_date\n\n\n\n\n2007-06-05"
  },
  {
    "objectID": "data-wrangling/dates.html#generating-new-dates",
    "href": "data-wrangling/dates.html#generating-new-dates",
    "title": "Dates",
    "section": "Generating new dates",
    "text": "Generating new dates\n\nPythonRSQL\n\n\n\nfrom datetime import date\n\n# datetime and date are not the same functions!\nnew_date = date(2023, 12, 13) \n\nprint(new_date)\n\n2023-12-13\n\n\n\n\n\nlibrary(lubridate)\n\nnew_date = make_date(year=2023, month=12, day=13)\nnew_date\n\n[1] \"2023-12-13\"\n\n\n\n\n\nselect\n  date('2023-12-31') as new_date;\n\n\n1 records\n\n\nnew_date\n\n\n\n\n2023-12-31"
  },
  {
    "objectID": "data-wrangling/dates.html#extracting-info-from-dates",
    "href": "data-wrangling/dates.html#extracting-info-from-dates",
    "title": "Dates",
    "section": "Extracting info from dates",
    "text": "Extracting info from dates\n\nPythonRSQL\n\n\n\nimport pandas as pd\nfrom datetime import datetime\n\n\n# Select specific columns\nselected_columns = orders[['quantity', 'order_date']].copy()\n\n# Extract year, month, and day\nselected_columns['year'] = selected_columns['order_date'].dt.year\nselected_columns['month'] = selected_columns['order_date'].dt.month\nselected_columns['day'] = selected_columns['order_date'].dt.day\n\n# Select first 5 rows\nresult = selected_columns.head(5)\n\n# Print the result\nprint(result)\n\n   quantity order_date  year  month  day\n0       2.0 2023-03-12  2023      3   12\n1       5.0 2023-09-08  2023      9    8\n2       3.0 2023-09-15  2023      9   15\n3       2.0 2023-01-19  2023      1   19\n4       1.0 2023-04-29  2023      4   29\n\n\n\n\n\nlibrary(lubridate)\n\norders %&gt;%\n  select(quantity, order_date) %&gt;% \n  mutate(\n    year = year(order_date),\n    month = month(order_date),\n    day = day(order_date)\n  ) %&gt;% \n  slice(1:5) %&gt;% \n  print()\n\n# A tibble: 5 × 5\n  quantity order_date           year month   day\n     &lt;dbl&gt; &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1        2 2023-03-12 00:00:00  2023     3    12\n2        5 2023-09-08 00:00:00  2023     9     8\n3        3 2023-09-15 00:00:00  2023     9    15\n4        2 2023-01-19 00:00:00  2023     1    19\n5        1 2023-04-29 00:00:00  2023     4    29\n\n\n\n\n\nselect\n  quantity,\n  order_date,\n  strftime('%Y', order_date) as year,\n  strftime('%m', order_date) as month,\n  strftime('%d', order_date) as day\nfrom orders\nlimit 5;\n\n\n5 records\n\n\nquantity\norder_date\nyear\nmonth\nday\n\n\n\n\n2\n2023-03-12\n2023\n03\n12\n\n\n5\n2023-09-08\n2023\n09\n08\n\n\n3\n2023-09-15\n2023\n09\n15\n\n\n2\n2023-01-19\n2023\n01\n19\n\n\n1\n2023-04-29\n2023\n04\n29"
  },
  {
    "objectID": "data-wrangling/dates.html#doing-math-with-dates",
    "href": "data-wrangling/dates.html#doing-math-with-dates",
    "title": "Dates",
    "section": "Doing math with dates",
    "text": "Doing math with dates\n\nPythonRSQL\n\n\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\n# Select specific columns\nselected_columns = orders[['quantity', 'order_date']].copy()\n\n# Extract year, month, and day\nselected_columns['plus_one_month'] = selected_columns['order_date'] + pd.DateOffset(months=1)\nselected_columns['plus_two_days'] = selected_columns['order_date'] + timedelta(days=2)\nselected_columns['minus_one_year'] = selected_columns['order_date'] - pd.DateOffset(years=1)\n\n# Select first 5 rows\nresult = selected_columns.head(5)\n\n# Print the result\nprint(result)\n\n   quantity order_date plus_one_month plus_two_days minus_one_year\n0       2.0 2023-03-12     2023-04-12    2023-03-14     2022-03-12\n1       5.0 2023-09-08     2023-10-08    2023-09-10     2022-09-08\n2       3.0 2023-09-15     2023-10-15    2023-09-17     2022-09-15\n3       2.0 2023-01-19     2023-02-19    2023-01-21     2022-01-19\n4       1.0 2023-04-29     2023-05-29    2023-05-01     2022-04-29\n\n\n\n\n\nlibrary(lubridate)\n\norders %&gt;%\n  select(quantity, order_date) %&gt;% \n  mutate(\n    plus_one_month = order_date + months(1),\n    plus_two_days = order_date + days(2),\n    minus_one_year = order_date - years(1)\n  ) %&gt;% \n  slice(1:5) %&gt;% \n  print()\n\n# A tibble: 5 × 5\n  quantity order_date          plus_one_month      plus_two_days      \n     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;             \n1        2 2023-03-12 00:00:00 2023-04-12 00:00:00 2023-03-14 00:00:00\n2        5 2023-09-08 00:00:00 2023-10-08 00:00:00 2023-09-10 00:00:00\n3        3 2023-09-15 00:00:00 2023-10-15 00:00:00 2023-09-17 00:00:00\n4        2 2023-01-19 00:00:00 2023-02-19 00:00:00 2023-01-21 00:00:00\n5        1 2023-04-29 00:00:00 2023-05-29 00:00:00 2023-05-01 00:00:00\n# ℹ 1 more variable: minus_one_year &lt;dttm&gt;\n\n\n\n\n\nselect\n  quantity,\n  order_date,\n  strftime('%Y-%m-%d', order_date, '+1 month') as plus_one_month,\n  strftime('%Y-%m-%d', order_date, '+2 days') as plus_two_days,\n  strftime('%Y-%m-%d', order_date, '-1 year') as minus_one_year\nfrom orders\nlimit 5;\n\n\n5 records\n\n\nquantity\norder_date\nplus_one_month\nplus_two_days\nminus_one_year\n\n\n\n\n2\n2023-03-12\n2023-04-12\n2023-03-14\n2022-03-12\n\n\n5\n2023-09-08\n2023-10-08\n2023-09-10\n2022-09-08\n\n\n3\n2023-09-15\n2023-10-15\n2023-09-17\n2022-09-15\n\n\n2\n2023-01-19\n2023-02-19\n2023-01-21\n2022-01-19\n\n\n1\n2023-04-29\n2023-05-29\n2023-05-01\n2022-04-29"
  },
  {
    "objectID": "data-wrangling/dates.html#get-todays-date",
    "href": "data-wrangling/dates.html#get-todays-date",
    "title": "Dates",
    "section": "Get today’s date",
    "text": "Get today’s date\n\nPythonRSQL\n\n\n\nfrom datetime import datetime\n\ncurrent_date = datetime.today().date()\n\nprint(current_date)\n\n2024-02-02\n\n\n\n\n\nlibrary(lubridate)\n\nprint(today())\n\n[1] \"2024-02-02\"\n\n\n\n\n\nSELECT date('now') AS current_date;\n\n\n1 records\n\n\ncurrent_date\n\n\n\n\n2024-02-02"
  },
  {
    "objectID": "data-wrangling/faking.html",
    "href": "data-wrangling/faking.html",
    "title": "Faking data",
    "section": "",
    "text": "R\n\n\n\nx = runif(n = 1000, min = 0, max = 200)\nboxplot(x)"
  },
  {
    "objectID": "data-wrangling/faking.html#generaring-a-uniform-distribution",
    "href": "data-wrangling/faking.html#generaring-a-uniform-distribution",
    "title": "Faking data",
    "section": "",
    "text": "R\n\n\n\nx = runif(n = 1000, min = 0, max = 200)\nboxplot(x)"
  },
  {
    "objectID": "data-wrangling/faking.html#faking-data-for-a-database",
    "href": "data-wrangling/faking.html#faking-data-for-a-database",
    "title": "Faking data",
    "section": "Faking data for a database",
    "text": "Faking data for a database\nFaking data is very useful for populating test databases. In fact, the script below was used to generate the example data with the tables customers, orders, suppliers and products used on this website.\n\nPython\n\n\n\nfrom faker import Faker\nimport pandas as pd\nimport random\n\nfake = Faker()\n\n# Generating customers\nnum_customers = 300\ncustomers = []\nfor _ in range(num_customers):\n    customers.append({\n        'customer_id': fake.uuid4(),\n        'customer_name': fake.name(),\n        'email': fake.email(),\n        'phone_number': fake.phone_number(),\n        'address': fake.address()\n    })\n\n# Generating suppliers\nnum_suppliers = 12\nsuppliers = []\nfor _ in range(num_suppliers):\n    suppliers.append({\n        'supplier_id': fake.uuid4(),\n        'supplier_name': fake.company(),\n        'supplier_email': fake.company_email(),\n        'phone_number': fake.phone_number(),\n        'address': fake.address()\n    })\n\n# Generating products\nnum_products = 30\nproducts = []\nfor i in range(num_products):\n    products.append({\n        'product_id': i + 1,\n        'product_name': fake.word(),\n        'product_description': fake.text(),\n        'price': round(random.uniform(10, 1000), 2),\n        'supplier_id': random.choice(suppliers)['supplier_id']\n    })\n\n# Generating orders\nnum_orders = 500\norders = []\nfor _ in range(num_orders):\n    customer = random.choice(customers)\n    order = {\n        'order_id': fake.uuid4(),\n        'customer_id': customer['customer_id'],\n        'product_id': random.choice(products)['product_id'],\n        'quantity': random.randint(1, 5),\n        'order_date': fake.date_this_year(),\n        'delivery_date': fake.date_between(start_date='today', end_date='+30d')\n    }\n    orders.append(order)\n\n# Creating DataFrames\ncustomers_df = pd.DataFrame(customers)\nsuppliers_df = pd.DataFrame(suppliers)\nproducts_df = pd.DataFrame(products)\norders_df = pd.DataFrame(orders)\n\n# Saving to Excel file\nwith pd.ExcelWriter('db.xlsx', engine='openpyxl') as writer:\n    customers_df.to_excel(writer, sheet_name='Customers', index=False)\n    suppliers_df.to_excel(writer, sheet_name='Suppliers', index=False)\n    products_df.to_excel(writer, sheet_name='Products', index=False)\n    orders_df.to_excel(writer, sheet_name='Orders', index=False)\n    \n\n# Create an SQLite database and establish connection\nconn = sqlite3.connect('db.sqlite')\ncursor = conn.cursor()\n\n# Create tables\ncursor.execute('''\n    CREATE TABLE Customers (\n        customer_id TEXT PRIMARY KEY,\n        customer_name TEXT,\n        email TEXT,\n        phone_number TEXT,\n        address TEXT\n    )\n''')\n\ncursor.execute('''\n    CREATE TABLE Suppliers (\n        supplier_id TEXT PRIMARY KEY,\n        supplier_name TEXT,\n        supplier_email TEXT,\n        phone_number TEXT,\n        address TEXT\n    )\n''')\n\ncursor.execute('''\n    CREATE TABLE Products (\n        product_id INTEGER PRIMARY KEY,\n        product_name TEXT,\n        product_description TEXT,\n        price REAL,\n        supplier_id TEXT,\n        FOREIGN KEY(supplier_id) REFERENCES Suppliers(supplier_id)\n    )\n''')\n\ncursor.execute('''\n    CREATE TABLE Orders (\n        order_id TEXT PRIMARY KEY,\n        customer_id TEXT,\n        product_id INTEGER,\n        quantity INTEGER,\n        order_date TEXT,\n        delivery_date TEXT,\n        FOREIGN KEY(customer_id) REFERENCES Customers(customer_id),\n        FOREIGN KEY(product_id) REFERENCES Products(product_id)\n    )\n''')\n\n# Insert data into tables\ncursor.executemany('''\n    INSERT INTO Customers (customer_id, customer_name, email, phone_number, address)\n    VALUES (:customer_id, :customer_name, :email, :phone_number, :address)\n''', customers)\n\ncursor.executemany('''\n    INSERT INTO Suppliers (supplier_id, supplier_name, supplier_email, phone_number, address)\n    VALUES (:supplier_id, :supplier_name, :supplier_email, :phone_number, :address)\n''', suppliers)\n\ncursor.executemany('''\n    INSERT INTO Products (product_id, product_name, product_description, price, supplier_id)\n    VALUES (:product_id, :product_name, :product_description, :price, :supplier_id)\n''', products)\n\ncursor.executemany('''\n    INSERT INTO Orders (order_id, customer_id, product_id, quantity, order_date, delivery_date)\n    VALUES (:order_id, :customer_id, :product_id, :quantity, :order_date, :delivery_date)\n''', orders)\n\n# Commit changes and close connection\nconn.commit()\nconn.close()"
  },
  {
    "objectID": "data-wrangling/joins.html",
    "href": "data-wrangling/joins.html",
    "title": "Joining",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\nresult = pd.merge(orders, products, on=\"product_id\", how=\"left\")\nresult = pd.merge(result, customers, on=\"customer_id\", how=\"left\")\nresult[\"tot_price\"] = result[\"price\"] * result[\"quantity\"]\nresult = result[[\"customer_name\", \"product_name\", \"quantity\", \"price\", \"tot_price\"]]\n\nprint(result)\n\n       customer_name product_name  quantity   price  tot_price\n0         Lisa Bates   conference       2.0  572.67    1145.34\n1     Olivia Kaufman         nice       5.0  648.89    3244.45\n2      Jeffrey Barry          day       3.0  874.70    2624.10\n3     Jessica Benson      develop       2.0  520.76    1041.52\n4        Tina Snyder          act       1.0  422.40     422.40\n..               ...          ...       ...     ...        ...\n495  Joshua Anderson     recently       2.0  182.29     364.58\n496  Robert Thornton         free       5.0  396.34    1981.70\n497       Eric Moore         fast       1.0  439.64     439.64\n498    Jaime Mcguire          our       3.0  677.56    2032.68\n499   Kimberly Brown          day       2.0  874.70    1749.40\n\n[500 rows x 5 columns]\n\n\n\n\n\n\n\nresult &lt;- orders %&gt;%\n  left_join(products, by = c(\"product_id\" = \"product_id\")) %&gt;% \n  left_join(customers, by = c(\"customer_id\" = \"customer_id\")) %&gt;%\n  mutate(tot_price = price*quantity) %&gt;% \n  select(customer_name, product_name, quantity, price, tot_price)\n  \nresult\n\n# A tibble: 500 × 5\n   customer_name  product_name quantity price tot_price\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Lisa Bates     conference          2 573.     1145. \n 2 Olivia Kaufman nice                5 649.     3244. \n 3 Jeffrey Barry  day                 3 875.     2624. \n 4 Jessica Benson develop             2 521.     1042. \n 5 Tina Snyder    act                 1 422.      422. \n 6 Thomas Campos  receive             3 224.      673. \n 7 Thomas Campos  field               1  72.3      72.3\n 8 Jill Price     free                4 396.     1585. \n 9 Debra Solis    recently            4 182.      729. \n10 Rebecca Wilson soon                5 215.     1074. \n# ℹ 490 more rows\n\n\n\n\n\n\ndf_joined &lt;- df1 %&gt;% left_join(df2, by=c('x1'='x2', 'y1'='y2'))\n\n\n\n\n\nselect \n    customers.customer_name, \n    products.product_name, \n    orders.quantity, \n    products.price, \n    products.price * orders.quantity as tot_price\nfrom orders\nleft join products on orders.product_id = products.product_id\nleft join customers on orders.customer_id = customers.customer_id;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_name\nproduct_name\nquantity\nprice\ntot_price\n\n\n\n\nLisa Bates\nconference\n2\n572.67\n1145.34\n\n\nOlivia Kaufman\nnice\n5\n648.89\n3244.45\n\n\nJeffrey Barry\nday\n3\n874.70\n2624.10\n\n\nJessica Benson\ndevelop\n2\n520.76\n1041.52\n\n\nTina Snyder\nact\n1\n422.40\n422.40\n\n\nThomas Campos\nreceive\n3\n224.33\n672.99\n\n\nThomas Campos\nfield\n1\n72.26\n72.26\n\n\nJill Price\nfree\n4\n396.34\n1585.36\n\n\nDebra Solis\nrecently\n4\n182.29\n729.16\n\n\nRebecca Wilson\nsoon\n5\n214.72\n1073.60"
  },
  {
    "objectID": "data-wrangling/joins.html#left-join",
    "href": "data-wrangling/joins.html#left-join",
    "title": "Joining",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\nresult = pd.merge(orders, products, on=\"product_id\", how=\"left\")\nresult = pd.merge(result, customers, on=\"customer_id\", how=\"left\")\nresult[\"tot_price\"] = result[\"price\"] * result[\"quantity\"]\nresult = result[[\"customer_name\", \"product_name\", \"quantity\", \"price\", \"tot_price\"]]\n\nprint(result)\n\n       customer_name product_name  quantity   price  tot_price\n0         Lisa Bates   conference       2.0  572.67    1145.34\n1     Olivia Kaufman         nice       5.0  648.89    3244.45\n2      Jeffrey Barry          day       3.0  874.70    2624.10\n3     Jessica Benson      develop       2.0  520.76    1041.52\n4        Tina Snyder          act       1.0  422.40     422.40\n..               ...          ...       ...     ...        ...\n495  Joshua Anderson     recently       2.0  182.29     364.58\n496  Robert Thornton         free       5.0  396.34    1981.70\n497       Eric Moore         fast       1.0  439.64     439.64\n498    Jaime Mcguire          our       3.0  677.56    2032.68\n499   Kimberly Brown          day       2.0  874.70    1749.40\n\n[500 rows x 5 columns]\n\n\n\n\n\n\n\nresult &lt;- orders %&gt;%\n  left_join(products, by = c(\"product_id\" = \"product_id\")) %&gt;% \n  left_join(customers, by = c(\"customer_id\" = \"customer_id\")) %&gt;%\n  mutate(tot_price = price*quantity) %&gt;% \n  select(customer_name, product_name, quantity, price, tot_price)\n  \nresult\n\n# A tibble: 500 × 5\n   customer_name  product_name quantity price tot_price\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Lisa Bates     conference          2 573.     1145. \n 2 Olivia Kaufman nice                5 649.     3244. \n 3 Jeffrey Barry  day                 3 875.     2624. \n 4 Jessica Benson develop             2 521.     1042. \n 5 Tina Snyder    act                 1 422.      422. \n 6 Thomas Campos  receive             3 224.      673. \n 7 Thomas Campos  field               1  72.3      72.3\n 8 Jill Price     free                4 396.     1585. \n 9 Debra Solis    recently            4 182.      729. \n10 Rebecca Wilson soon                5 215.     1074. \n# ℹ 490 more rows\n\n\n\n\n\n\ndf_joined &lt;- df1 %&gt;% left_join(df2, by=c('x1'='x2', 'y1'='y2'))\n\n\n\n\n\nselect \n    customers.customer_name, \n    products.product_name, \n    orders.quantity, \n    products.price, \n    products.price * orders.quantity as tot_price\nfrom orders\nleft join products on orders.product_id = products.product_id\nleft join customers on orders.customer_id = customers.customer_id;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_name\nproduct_name\nquantity\nprice\ntot_price\n\n\n\n\nLisa Bates\nconference\n2\n572.67\n1145.34\n\n\nOlivia Kaufman\nnice\n5\n648.89\n3244.45\n\n\nJeffrey Barry\nday\n3\n874.70\n2624.10\n\n\nJessica Benson\ndevelop\n2\n520.76\n1041.52\n\n\nTina Snyder\nact\n1\n422.40\n422.40\n\n\nThomas Campos\nreceive\n3\n224.33\n672.99\n\n\nThomas Campos\nfield\n1\n72.26\n72.26\n\n\nJill Price\nfree\n4\n396.34\n1585.36\n\n\nDebra Solis\nrecently\n4\n182.29\n729.16\n\n\nRebecca Wilson\nsoon\n5\n214.72\n1073.60"
  },
  {
    "objectID": "data-wrangling/altering.html",
    "href": "data-wrangling/altering.html",
    "title": "Altering",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\n# Assuming customers is your DataFrame\ndf_reordered = customers[['customer_name'] + list(customers.columns.drop('customer_name'))]\ndf_reordered\n\n          customer_name  ...                                            address\n0        Anthony Guerra  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n1      Elizabeth Newton  ...  5745 Gomez Trafficway Suite 809\\nEast Nathanbe...\n2    Gabrielle Chandler  ...  165 Richards Heights Suite 499\\nEllisview, AR ...\n3         Megan Elliott  ...    049 Andrew Drive Apt. 813\\nTurnerside, WI 61461\n4         Kristin James  ...  2048 Derrick Neck Suite 102\\nSouth Stephaniemo...\n..                  ...  ...                                                ...\n295    Gloria Miller MD  ...  02104 Jason Mountain Suite 530\\nWest Marcussta...\n296      Natalie Butler  ...  014 Wheeler Island Suite 192\\nWebsterport, IA ...\n297    Kimberly Johnson  ...        4609 Porter Mill\\nSouth Donaldton, ID 77125\n298     Joshua Trujillo  ...  175 Bush Streets Apt. 625\\nAmandaborough, AK 8...\n299       Alyssa Howard  ...             81103 Colin Mount\\nJamesstad, NV 46158\n\n[300 rows x 5 columns]\n\n\n\n\n\ndf_reordered = customers %&gt;% \n  select(customer_name, email, everything()) # move name to beginning\n\ndf_reordered_2 = customers %&gt;% \n  relocate(customer_id, .after = last_col()) # move name to end\n\ndf_reordered_3 = customers %&gt;% \n  relocate(email, .before = customer_name) # move name to end\n\ndf_reordered_3\n\n# A tibble: 300 × 5\n   customer_id                          email customer_name phone_number address\n   &lt;chr&gt;                                &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;  \n 1 8ab2552b-e4c9-471d-bdf1-ff6e225860e1 jane… Anthony Guer… 659.461.7580 \"963 R…\n 2 57a7c826-f022-4f6c-9a21-81d5f99da142 xben… Elizabeth Ne… 001-555-635… \"5745 …\n 3 6ff9e916-8596-4fc6-a224-fd1a6e163837 cjac… Gabrielle Ch… 943-259-602… \"165 R…\n 4 0706dd55-3603-4985-8136-76872925d39c ryan… Megan Elliott (668)271-02… \"049 A…\n 5 aa8662fc-f9c0-4e56-a498-693df376020a broo… Kristin James 815-164-705… \"2048 …\n 6 02306791-9603-4a34-b4c8-e6a7019fbe7f henr… Tyler Greene  0769607875   \"344 D…\n 7 43976a56-94e0-4769-b7e8-c9f15d4a8511 caro… Tammy Schwar… (877)554-18… \"0870 …\n 8 ae209764-c4b2-4c27-82f7-d41f7e71aded king… Sarah Kennedy 001-784-128… \"609 S…\n 9 0230e24e-48c2-44ec-bbe8-f6aff9029bed mich… Kevin Hodges  001-969-144… \"23923…\n10 6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7 will… Nathan Mitch… 085.010.406… \"053 B…\n# ℹ 290 more rows\n\n\n\n\n\nSELECT\n  customer_name, -- just choose the order in the select statement\n  customer_id,\n  email,\n  phone_number\nFROM \n  customers;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\ncustomer_name\ncustomer_id\nemail\nphone_number\n\n\n\n\nAnthony Guerra\n8ab2552b-e4c9-471d-bdf1-ff6e225860e1\njanet90@example.net\n659.461.7580\n\n\nElizabeth Newton\n57a7c826-f022-4f6c-9a21-81d5f99da142\nxbentley@example.com\n001-555-635-9586x3938\n\n\nGabrielle Chandler\n6ff9e916-8596-4fc6-a224-fd1a6e163837\ncjackson@example.net\n943-259-6029x806\n\n\nMegan Elliott\n0706dd55-3603-4985-8136-76872925d39c\nryan87@example.com\n(668)271-0295x004\n\n\nKristin James\naa8662fc-f9c0-4e56-a498-693df376020a\nbrooksamber@example.com\n815-164-7057x78004\n\n\nTyler Greene\n02306791-9603-4a34-b4c8-e6a7019fbe7f\nhenry78@example.net\n0769607875\n\n\nTammy Schwartz\n43976a56-94e0-4769-b7e8-c9f15d4a8511\ncarolthomas@example.org\n(877)554-1860x64446\n\n\nSarah Kennedy\nae209764-c4b2-4c27-82f7-d41f7e71aded\nkingrobert@example.net\n001-784-128-8396\n\n\nKevin Hodges\n0230e24e-48c2-44ec-bbe8-f6aff9029bed\nmichael74@example.org\n001-969-144-3784\n\n\nNathan Mitchell\n6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7\nwilliamsrussell@example.net\n085.010.4067x31103"
  },
  {
    "objectID": "data-wrangling/altering.html#move-columns",
    "href": "data-wrangling/altering.html#move-columns",
    "title": "Altering",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\n# Assuming customers is your DataFrame\ndf_reordered = customers[['customer_name'] + list(customers.columns.drop('customer_name'))]\ndf_reordered\n\n          customer_name  ...                                            address\n0        Anthony Guerra  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n1      Elizabeth Newton  ...  5745 Gomez Trafficway Suite 809\\nEast Nathanbe...\n2    Gabrielle Chandler  ...  165 Richards Heights Suite 499\\nEllisview, AR ...\n3         Megan Elliott  ...    049 Andrew Drive Apt. 813\\nTurnerside, WI 61461\n4         Kristin James  ...  2048 Derrick Neck Suite 102\\nSouth Stephaniemo...\n..                  ...  ...                                                ...\n295    Gloria Miller MD  ...  02104 Jason Mountain Suite 530\\nWest Marcussta...\n296      Natalie Butler  ...  014 Wheeler Island Suite 192\\nWebsterport, IA ...\n297    Kimberly Johnson  ...        4609 Porter Mill\\nSouth Donaldton, ID 77125\n298     Joshua Trujillo  ...  175 Bush Streets Apt. 625\\nAmandaborough, AK 8...\n299       Alyssa Howard  ...             81103 Colin Mount\\nJamesstad, NV 46158\n\n[300 rows x 5 columns]\n\n\n\n\n\ndf_reordered = customers %&gt;% \n  select(customer_name, email, everything()) # move name to beginning\n\ndf_reordered_2 = customers %&gt;% \n  relocate(customer_id, .after = last_col()) # move name to end\n\ndf_reordered_3 = customers %&gt;% \n  relocate(email, .before = customer_name) # move name to end\n\ndf_reordered_3\n\n# A tibble: 300 × 5\n   customer_id                          email customer_name phone_number address\n   &lt;chr&gt;                                &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;  \n 1 8ab2552b-e4c9-471d-bdf1-ff6e225860e1 jane… Anthony Guer… 659.461.7580 \"963 R…\n 2 57a7c826-f022-4f6c-9a21-81d5f99da142 xben… Elizabeth Ne… 001-555-635… \"5745 …\n 3 6ff9e916-8596-4fc6-a224-fd1a6e163837 cjac… Gabrielle Ch… 943-259-602… \"165 R…\n 4 0706dd55-3603-4985-8136-76872925d39c ryan… Megan Elliott (668)271-02… \"049 A…\n 5 aa8662fc-f9c0-4e56-a498-693df376020a broo… Kristin James 815-164-705… \"2048 …\n 6 02306791-9603-4a34-b4c8-e6a7019fbe7f henr… Tyler Greene  0769607875   \"344 D…\n 7 43976a56-94e0-4769-b7e8-c9f15d4a8511 caro… Tammy Schwar… (877)554-18… \"0870 …\n 8 ae209764-c4b2-4c27-82f7-d41f7e71aded king… Sarah Kennedy 001-784-128… \"609 S…\n 9 0230e24e-48c2-44ec-bbe8-f6aff9029bed mich… Kevin Hodges  001-969-144… \"23923…\n10 6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7 will… Nathan Mitch… 085.010.406… \"053 B…\n# ℹ 290 more rows\n\n\n\n\n\nSELECT\n  customer_name, -- just choose the order in the select statement\n  customer_id,\n  email,\n  phone_number\nFROM \n  customers;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\ncustomer_name\ncustomer_id\nemail\nphone_number\n\n\n\n\nAnthony Guerra\n8ab2552b-e4c9-471d-bdf1-ff6e225860e1\njanet90@example.net\n659.461.7580\n\n\nElizabeth Newton\n57a7c826-f022-4f6c-9a21-81d5f99da142\nxbentley@example.com\n001-555-635-9586x3938\n\n\nGabrielle Chandler\n6ff9e916-8596-4fc6-a224-fd1a6e163837\ncjackson@example.net\n943-259-6029x806\n\n\nMegan Elliott\n0706dd55-3603-4985-8136-76872925d39c\nryan87@example.com\n(668)271-0295x004\n\n\nKristin James\naa8662fc-f9c0-4e56-a498-693df376020a\nbrooksamber@example.com\n815-164-7057x78004\n\n\nTyler Greene\n02306791-9603-4a34-b4c8-e6a7019fbe7f\nhenry78@example.net\n0769607875\n\n\nTammy Schwartz\n43976a56-94e0-4769-b7e8-c9f15d4a8511\ncarolthomas@example.org\n(877)554-1860x64446\n\n\nSarah Kennedy\nae209764-c4b2-4c27-82f7-d41f7e71aded\nkingrobert@example.net\n001-784-128-8396\n\n\nKevin Hodges\n0230e24e-48c2-44ec-bbe8-f6aff9029bed\nmichael74@example.org\n001-969-144-3784\n\n\nNathan Mitchell\n6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7\nwilliamsrussell@example.net\n085.010.4067x31103"
  },
  {
    "objectID": "data-wrangling/altering.html#rename-columns",
    "href": "data-wrangling/altering.html#rename-columns",
    "title": "Altering",
    "section": "Rename columns",
    "text": "Rename columns\n\nPythonRSQL\n\n\n\nimport pandas as pd\n\n# rename the 'customer_name' column to 'Name'\nrenamed = customers.rename(columns={'customer_name': 'Name'})\n\nprint(renamed)\n\n                              customer_id  ...                                            address\n0    8ab2552b-e4c9-471d-bdf1-ff6e225860e1  ...  963 Regina Bridge Apt. 279\\nPort Richard, NJ 4...\n1    57a7c826-f022-4f6c-9a21-81d5f99da142  ...  5745 Gomez Trafficway Suite 809\\nEast Nathanbe...\n2    6ff9e916-8596-4fc6-a224-fd1a6e163837  ...  165 Richards Heights Suite 499\\nEllisview, AR ...\n3    0706dd55-3603-4985-8136-76872925d39c  ...    049 Andrew Drive Apt. 813\\nTurnerside, WI 61461\n4    aa8662fc-f9c0-4e56-a498-693df376020a  ...  2048 Derrick Neck Suite 102\\nSouth Stephaniemo...\n..                                    ...  ...                                                ...\n295  a3a3e33d-1dc3-4fa2-85ba-e0c4f80bae59  ...  02104 Jason Mountain Suite 530\\nWest Marcussta...\n296  4bf8b39f-c9f7-4791-9bf7-3a0b4dd6cf34  ...  014 Wheeler Island Suite 192\\nWebsterport, IA ...\n297  00a56cfb-8b3f-460f-871f-49ba5de1816b  ...        4609 Porter Mill\\nSouth Donaldton, ID 77125\n298  48500481-5356-43ff-bc92-8b859c4b301e  ...  175 Bush Streets Apt. 625\\nAmandaborough, AK 8...\n299  982ff64b-0d2d-4e84-8b92-be588de9affa  ...             81103 Colin Mount\\nJamesstad, NV 46158\n\n[300 rows x 5 columns]\n\n\n\n\n\nrenamed = customers %&gt;% \n  rename(\n    Name = customer_name,\n    E_Mail = email\n  )\nrenamed\n\n# A tibble: 300 × 5\n   customer_id                          Name         E_Mail phone_number address\n   &lt;chr&gt;                                &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;  \n 1 8ab2552b-e4c9-471d-bdf1-ff6e225860e1 Anthony Gue… janet… 659.461.7580 \"963 R…\n 2 57a7c826-f022-4f6c-9a21-81d5f99da142 Elizabeth N… xbent… 001-555-635… \"5745 …\n 3 6ff9e916-8596-4fc6-a224-fd1a6e163837 Gabrielle C… cjack… 943-259-602… \"165 R…\n 4 0706dd55-3603-4985-8136-76872925d39c Megan Ellio… ryan8… (668)271-02… \"049 A…\n 5 aa8662fc-f9c0-4e56-a498-693df376020a Kristin Jam… brook… 815-164-705… \"2048 …\n 6 02306791-9603-4a34-b4c8-e6a7019fbe7f Tyler Greene henry… 0769607875   \"344 D…\n 7 43976a56-94e0-4769-b7e8-c9f15d4a8511 Tammy Schwa… carol… (877)554-18… \"0870 …\n 8 ae209764-c4b2-4c27-82f7-d41f7e71aded Sarah Kenne… kingr… 001-784-128… \"609 S…\n 9 0230e24e-48c2-44ec-bbe8-f6aff9029bed Kevin Hodges micha… 001-969-144… \"23923…\n10 6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7 Nathan Mitc… willi… 085.010.406… \"053 B…\n# ℹ 290 more rows\n\n# using df %&gt;% janitor::clean_names() u can get rid of white spaces, dots, etc.\n\n\n\n\nSELECT\n  customer_name as Name,\n  customer_id,\n  email,\n  phone_number\nFROM \n  customers;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nName\ncustomer_id\nemail\nphone_number\n\n\n\n\nAnthony Guerra\n8ab2552b-e4c9-471d-bdf1-ff6e225860e1\njanet90@example.net\n659.461.7580\n\n\nElizabeth Newton\n57a7c826-f022-4f6c-9a21-81d5f99da142\nxbentley@example.com\n001-555-635-9586x3938\n\n\nGabrielle Chandler\n6ff9e916-8596-4fc6-a224-fd1a6e163837\ncjackson@example.net\n943-259-6029x806\n\n\nMegan Elliott\n0706dd55-3603-4985-8136-76872925d39c\nryan87@example.com\n(668)271-0295x004\n\n\nKristin James\naa8662fc-f9c0-4e56-a498-693df376020a\nbrooksamber@example.com\n815-164-7057x78004\n\n\nTyler Greene\n02306791-9603-4a34-b4c8-e6a7019fbe7f\nhenry78@example.net\n0769607875\n\n\nTammy Schwartz\n43976a56-94e0-4769-b7e8-c9f15d4a8511\ncarolthomas@example.org\n(877)554-1860x64446\n\n\nSarah Kennedy\nae209764-c4b2-4c27-82f7-d41f7e71aded\nkingrobert@example.net\n001-784-128-8396\n\n\nKevin Hodges\n0230e24e-48c2-44ec-bbe8-f6aff9029bed\nmichael74@example.org\n001-969-144-3784\n\n\nNathan Mitchell\n6f7a4a4e-a87a-4f6d-927f-a8e2978b00a7\nwilliamsrussell@example.net\n085.010.4067x31103"
  },
  {
    "objectID": "data-wrangling/exploring.html",
    "href": "data-wrangling/exploring.html",
    "title": "Exploring data",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\nsuppliers.head()\n\n                            supplier_id  ...                                            address\n0  e0fd9397-2fbc-49d4-8d1f-c7708443d1ac  ...        6480 Henderson Ranch\\nPort Nicole, RI 15164\n1  70e3f6f7-6d0d-4239-8a23-38bea2219425  ...                   PSC 5322, Box 2350\\nAPO AP 44327\n2  a520d32a-e6dc-4051-b11b-58db53efc241  ...     24075 Garcia Hill Suite 900\\nMarkton, NH 13547\n3  4f63e2c8-181b-435a-a30c-4f2bd4f03cf0  ...  96401 Fernandez Rest Suite 354\\nDavidmouth, NM...\n4  278d2735-a01b-4993-a648-e81a38593478  ...                   PSC 5358, Box 9304\\nAPO AE 35414\n\n[5 rows x 5 columns]\n\nsuppliers.tail()\n\n                             supplier_id  ...                                            address\n7   b7fe0aa1-0742-4a23-839e-5ecb293b8348  ...         37237 Johnson Squares\\nCoryville, MN 02964\n8   c04e4e30-f25d-48f5-95f1-ac67d152eff3  ...           8377 Arnold Summit\\nPerezhaven, UT 40182\n9   c450c364-da66-41fc-8a20-ef8c81fe0092  ...  28359 Becky Forges Suite 510\\nMcclureton, WV 0...\n10  38a0efd5-a37a-4dc4-81db-f1285aec5bc0  ...          82367 Andrew Course\\nAlyssaberg, FL 82783\n11  9fd06f8f-9112-40b1-b39c-ef302a7a667a  ...      6548 Terry Junctions\\nSouth Kristin, KS 64925\n\n[5 rows x 5 columns]\n\n\n\n\n\nhead(suppliers)\n\n# A tibble: 6 × 5\n  supplier_id                  supplier_name supplier_email phone_number address\n  &lt;chr&gt;                        &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;  \n1 e0fd9397-2fbc-49d4-8d1f-c77… Scott Inc     terri64@denni… +1-624-789-… \"6480 …\n2 70e3f6f7-6d0d-4239-8a23-38b… Stephenson-G… allison76@she… 538.519.5134 \"PSC 5…\n3 a520d32a-e6dc-4051-b11b-58d… Morgan, Lars… qstout@willia… 321-639-9342 \"24075…\n4 4f63e2c8-181b-435a-a30c-4f2… Lewis, Ochoa… theresajacobs… 001-899-743… \"96401…\n5 278d2735-a01b-4993-a648-e81… Taylor, Brew… christinemors… 229.847.0926 \"PSC 5…\n6 a5dafaa6-a0af-404e-be90-245… Jones-Liu     lawsonbrian@s… +1-449-946-… \"USNV …\n\ntail(suppliers)\n\n# A tibble: 6 × 5\n  supplier_id                  supplier_name supplier_email phone_number address\n  &lt;chr&gt;                        &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;  \n1 17a4736d-6fe6-404c-bf0c-20c… Flowers Inc   davidlane@dav… 796-014-6882 \"858 C…\n2 b7fe0aa1-0742-4a23-839e-5ec… Bennett, Ros… ithompson@bro… (144)217-34… \"37237…\n3 c04e4e30-f25d-48f5-95f1-ac6… Richards-Lit… moodybrittany… +1-865-223-… \"8377 …\n4 c450c364-da66-41fc-8a20-ef8… Patel, Barre… lucas13@duran… +1-779-999-… \"28359…\n5 38a0efd5-a37a-4dc4-81db-f12… Jackson-Duran turnergloria@… 897-080-050… \"82367…\n6 9fd06f8f-9112-40b1-b39c-ef3… Galvan-Alexa… curtis73@coop… +1-834-163-… \"6548 …\n\n\n\n\n\nselect * from suppliers order by supplier_id limit 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\nsupplier_id\nsupplier_name\nsupplier_email\nphone_number\naddress\n\n\n\n\n17a4736d-6fe6-404c-bf0c-20c9ac3a7b13\nFlowers Inc\ndavidlane@davis.com\n796-014-6882\n858 Colton Via Apt. 071\n\n\nLake Michelleshire, PA 08254\n\n\n\n\n\n\n278d2735-a01b-4993-a648-e81a38593478\nTaylor, Brewer and Richard\nchristinemorse@shields.biz\n229.847.0926\nPSC 5358, Box 9304\n\n\nAPO AE 35414\n\n\n\n\n\n\n38a0efd5-a37a-4dc4-81db-f1285aec5bc0\nJackson-Duran\nturnergloria@williams.org\n897-080-0502x231\n82367 Andrew Course\n\n\nAlyssaberg, FL 82783\n\n\n\n\n\n\n4f63e2c8-181b-435a-a30c-4f2bd4f03cf0\nLewis, Ochoa and Hendricks\ntheresajacobs@mills-may.com\n001-899-743-1582x55248\n96401 Fernandez Rest Suite 354\n\n\nDavidmouth, NM 75675\n\n\n\n\n\n\n70e3f6f7-6d0d-4239-8a23-38bea2219425\nStephenson-Gonzalez\nallison76@shepherd.com\n538.519.5134\nPSC 5322, Box 2350\n\n\nAPO AP 44327\n\n\n\n\n\n\n\n\n\n\nselect * from suppliers order by supplier_id desc limit 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\nsupplier_id\nsupplier_name\nsupplier_email\nphone_number\naddress\n\n\n\n\ne0fd9397-2fbc-49d4-8d1f-c7708443d1ac\nScott Inc\nterri64@dennis.com\n+1-624-789-0312x85642\n6480 Henderson Ranch\n\n\nPort Nicole, RI 15164\n\n\n\n\n\n\nc450c364-da66-41fc-8a20-ef8c81fe0092\nPatel, Barrett and Mullen\nlucas13@duran.info\n+1-779-999-3397x19609\n28359 Becky Forges Suite 510\n\n\nMcclureton, WV 04781\n\n\n\n\n\n\nc04e4e30-f25d-48f5-95f1-ac67d152eff3\nRichards-Little\nmoodybrittany@hamilton-juarez.com\n+1-865-223-0136x5947\n8377 Arnold Summit\n\n\nPerezhaven, UT 40182\n\n\n\n\n\n\nb7fe0aa1-0742-4a23-839e-5ecb293b8348\nBennett, Rose and Curry\nithompson@brown-dixon.com\n(144)217-3402x705\n37237 Johnson Squares\n\n\nCoryville, MN 02964\n\n\n\n\n\n\na5dafaa6-a0af-404e-be90-245a62ff8a52\nJones-Liu\nlawsonbrian@stone.net\n+1-449-946-5306x917\nUSNV Martinez\n\n\nFPO AA 68692"
  },
  {
    "objectID": "data-wrangling/exploring.html#show-top-last-rows",
    "href": "data-wrangling/exploring.html#show-top-last-rows",
    "title": "Exploring data",
    "section": "",
    "text": "PythonRSQL\n\n\n\nimport pandas as pd\n\nsuppliers.head()\n\n                            supplier_id  ...                                            address\n0  e0fd9397-2fbc-49d4-8d1f-c7708443d1ac  ...        6480 Henderson Ranch\\nPort Nicole, RI 15164\n1  70e3f6f7-6d0d-4239-8a23-38bea2219425  ...                   PSC 5322, Box 2350\\nAPO AP 44327\n2  a520d32a-e6dc-4051-b11b-58db53efc241  ...     24075 Garcia Hill Suite 900\\nMarkton, NH 13547\n3  4f63e2c8-181b-435a-a30c-4f2bd4f03cf0  ...  96401 Fernandez Rest Suite 354\\nDavidmouth, NM...\n4  278d2735-a01b-4993-a648-e81a38593478  ...                   PSC 5358, Box 9304\\nAPO AE 35414\n\n[5 rows x 5 columns]\n\nsuppliers.tail()\n\n                             supplier_id  ...                                            address\n7   b7fe0aa1-0742-4a23-839e-5ecb293b8348  ...         37237 Johnson Squares\\nCoryville, MN 02964\n8   c04e4e30-f25d-48f5-95f1-ac67d152eff3  ...           8377 Arnold Summit\\nPerezhaven, UT 40182\n9   c450c364-da66-41fc-8a20-ef8c81fe0092  ...  28359 Becky Forges Suite 510\\nMcclureton, WV 0...\n10  38a0efd5-a37a-4dc4-81db-f1285aec5bc0  ...          82367 Andrew Course\\nAlyssaberg, FL 82783\n11  9fd06f8f-9112-40b1-b39c-ef302a7a667a  ...      6548 Terry Junctions\\nSouth Kristin, KS 64925\n\n[5 rows x 5 columns]\n\n\n\n\n\nhead(suppliers)\n\n# A tibble: 6 × 5\n  supplier_id                  supplier_name supplier_email phone_number address\n  &lt;chr&gt;                        &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;  \n1 e0fd9397-2fbc-49d4-8d1f-c77… Scott Inc     terri64@denni… +1-624-789-… \"6480 …\n2 70e3f6f7-6d0d-4239-8a23-38b… Stephenson-G… allison76@she… 538.519.5134 \"PSC 5…\n3 a520d32a-e6dc-4051-b11b-58d… Morgan, Lars… qstout@willia… 321-639-9342 \"24075…\n4 4f63e2c8-181b-435a-a30c-4f2… Lewis, Ochoa… theresajacobs… 001-899-743… \"96401…\n5 278d2735-a01b-4993-a648-e81… Taylor, Brew… christinemors… 229.847.0926 \"PSC 5…\n6 a5dafaa6-a0af-404e-be90-245… Jones-Liu     lawsonbrian@s… +1-449-946-… \"USNV …\n\ntail(suppliers)\n\n# A tibble: 6 × 5\n  supplier_id                  supplier_name supplier_email phone_number address\n  &lt;chr&gt;                        &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;  \n1 17a4736d-6fe6-404c-bf0c-20c… Flowers Inc   davidlane@dav… 796-014-6882 \"858 C…\n2 b7fe0aa1-0742-4a23-839e-5ec… Bennett, Ros… ithompson@bro… (144)217-34… \"37237…\n3 c04e4e30-f25d-48f5-95f1-ac6… Richards-Lit… moodybrittany… +1-865-223-… \"8377 …\n4 c450c364-da66-41fc-8a20-ef8… Patel, Barre… lucas13@duran… +1-779-999-… \"28359…\n5 38a0efd5-a37a-4dc4-81db-f12… Jackson-Duran turnergloria@… 897-080-050… \"82367…\n6 9fd06f8f-9112-40b1-b39c-ef3… Galvan-Alexa… curtis73@coop… +1-834-163-… \"6548 …\n\n\n\n\n\nselect * from suppliers order by supplier_id limit 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\nsupplier_id\nsupplier_name\nsupplier_email\nphone_number\naddress\n\n\n\n\n17a4736d-6fe6-404c-bf0c-20c9ac3a7b13\nFlowers Inc\ndavidlane@davis.com\n796-014-6882\n858 Colton Via Apt. 071\n\n\nLake Michelleshire, PA 08254\n\n\n\n\n\n\n278d2735-a01b-4993-a648-e81a38593478\nTaylor, Brewer and Richard\nchristinemorse@shields.biz\n229.847.0926\nPSC 5358, Box 9304\n\n\nAPO AE 35414\n\n\n\n\n\n\n38a0efd5-a37a-4dc4-81db-f1285aec5bc0\nJackson-Duran\nturnergloria@williams.org\n897-080-0502x231\n82367 Andrew Course\n\n\nAlyssaberg, FL 82783\n\n\n\n\n\n\n4f63e2c8-181b-435a-a30c-4f2bd4f03cf0\nLewis, Ochoa and Hendricks\ntheresajacobs@mills-may.com\n001-899-743-1582x55248\n96401 Fernandez Rest Suite 354\n\n\nDavidmouth, NM 75675\n\n\n\n\n\n\n70e3f6f7-6d0d-4239-8a23-38bea2219425\nStephenson-Gonzalez\nallison76@shepherd.com\n538.519.5134\nPSC 5322, Box 2350\n\n\nAPO AP 44327\n\n\n\n\n\n\n\n\n\n\nselect * from suppliers order by supplier_id desc limit 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\nsupplier_id\nsupplier_name\nsupplier_email\nphone_number\naddress\n\n\n\n\ne0fd9397-2fbc-49d4-8d1f-c7708443d1ac\nScott Inc\nterri64@dennis.com\n+1-624-789-0312x85642\n6480 Henderson Ranch\n\n\nPort Nicole, RI 15164\n\n\n\n\n\n\nc450c364-da66-41fc-8a20-ef8c81fe0092\nPatel, Barrett and Mullen\nlucas13@duran.info\n+1-779-999-3397x19609\n28359 Becky Forges Suite 510\n\n\nMcclureton, WV 04781\n\n\n\n\n\n\nc04e4e30-f25d-48f5-95f1-ac67d152eff3\nRichards-Little\nmoodybrittany@hamilton-juarez.com\n+1-865-223-0136x5947\n8377 Arnold Summit\n\n\nPerezhaven, UT 40182\n\n\n\n\n\n\nb7fe0aa1-0742-4a23-839e-5ecb293b8348\nBennett, Rose and Curry\nithompson@brown-dixon.com\n(144)217-3402x705\n37237 Johnson Squares\n\n\nCoryville, MN 02964\n\n\n\n\n\n\na5dafaa6-a0af-404e-be90-245a62ff8a52\nJones-Liu\nlawsonbrian@stone.net\n+1-449-946-5306x917\nUSNV Martinez\n\n\nFPO AA 68692"
  },
  {
    "objectID": "data-wrangling/exploring.html#check-dimensions-structure",
    "href": "data-wrangling/exploring.html#check-dimensions-structure",
    "title": "Exploring data",
    "section": "Check dimensions & structure",
    "text": "Check dimensions & structure\n\nPythonRSQL\n\n\n\nimport pandas as pd\n\nnum_rows = len(orders)\nprint(f\"Number of rows: {num_rows}\")\n\nNumber of rows: 500\n\nnum_cols = len(orders.columns)\nprint(f\"Number of columns: {num_cols}\")\n\nNumber of columns: 6\n\nprint(orders.info()) # Structure of the df\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   order_id       500 non-null    object        \n 1   customer_id    500 non-null    object        \n 2   product_id     500 non-null    float64       \n 3   quantity       500 non-null    float64       \n 4   order_date     500 non-null    datetime64[ns]\n 5   delivery_date  500 non-null    datetime64[ns]\ndtypes: datetime64[ns](2), float64(2), object(2)\nmemory usage: 23.6+ KB\nNone\n\nprint(orders.describe()) # Summary statistics\n\n       product_id  ...        delivery_date\ncount  500.000000  ...                  500\nmean    15.232000  ...  2023-12-10 20:09:36\nmin      1.000000  ...  2023-11-26 00:00:00\n25%      8.000000  ...  2023-12-03 00:00:00\n50%     15.000000  ...  2023-12-11 00:00:00\n75%     23.000000  ...  2023-12-19 00:00:00\nmax     30.000000  ...  2023-12-25 00:00:00\nstd      8.758516  ...                  NaN\n\n[8 rows x 4 columns]\n\n\n\n\n\nnrow(orders)\n\n[1] 500\n\nncol(orders)\n\n[1] 6\n\nstr(orders)\n\ntibble [500 × 6] (S3: tbl_df/tbl/data.frame)\n $ order_id     : chr [1:500] \"4ae1b891-4576-4c92-9d33-183788716d30\" \"7f447e86-16be-42ff-a32c-5ad13f06adbf\" \"aab0b35f-6e75-49d7-a85f-ce375a941d36\" \"a51c1a94-911d-4415-a4ef-8559b67cbdcf\" ...\n $ customer_id  : chr [1:500] \"97f268da-60bd-48b0-b51b-5648641c04cb\" \"36cd45dd-3b82-4672-91ed-ea3bd821024a\" \"5b4e2a6e-dab7-415b-badb-1e4ef26155ac\" \"086af576-2f26-453c-ac15-5a597d7b9894\" ...\n $ product_id   : num [1:500] 27 30 6 12 11 17 28 20 22 8 ...\n $ quantity     : num [1:500] 2 5 3 2 1 3 1 4 4 5 ...\n $ order_date   : POSIXct[1:500], format: \"2023-03-12\" \"2023-09-08\" ...\n $ delivery_date: POSIXct[1:500], format: \"2023-12-09\" \"2023-12-02\" ...\n\nsummary(orders)\n\n   order_id         customer_id          product_id       quantity    \n Length:500         Length:500         Min.   : 1.00   Min.   :1.000  \n Class :character   Class :character   1st Qu.: 8.00   1st Qu.:2.000  \n Mode  :character   Mode  :character   Median :15.00   Median :3.000  \n                                       Mean   :15.23   Mean   :3.144  \n                                       3rd Qu.:23.00   3rd Qu.:4.000  \n                                       Max.   :30.00   Max.   :5.000  \n   order_date                  delivery_date                \n Min.   :2023-01-01 00:00:00   Min.   :2023-11-26 00:00:00  \n 1st Qu.:2023-03-16 18:00:00   1st Qu.:2023-12-03 00:00:00  \n Median :2023-06-10 12:00:00   Median :2023-12-11 00:00:00  \n Mean   :2023-06-10 22:36:28   Mean   :2023-12-10 20:09:36  \n 3rd Qu.:2023-08-31 06:00:00   3rd Qu.:2023-12-19 00:00:00  \n Max.   :2023-11-25 00:00:00   Max.   :2023-12-25 00:00:00  \n\n\n\n\n\nselect count(*) as num_rows from orders;\n\n-- Adapt the lines below depending on used DB\n--select\n--  count(*) as num_cols\n--from\n--  information_schema.columns\n--where 0=0\n--  and table_name = 'orders';\n\n\n1 records\n\n\nnum_rows\n\n\n\n\n500"
  },
  {
    "objectID": "data-wrangling/choices.html",
    "href": "data-wrangling/choices.html",
    "title": "Choices",
    "section": "",
    "text": "Choices enable you to run different code based on conditions / input. The most basic form of choices is the if statement.\n\nPythonRSQL\n\n\n\nz = 4\nif z % 2 == 0:\n    print(\"z is even\")\nelse:\n    print(\"z is uneven\")\n\nz is even\n\n\n\n\nin one line\n\nz = 4\nif (z %% 2 == 0) print(\"z is even\") else print(\"z is odd\")\n\n[1] \"z is even\"\n\n\nin several lines\n\nz = 5\nif (z %% 2 == 0) {\n  print(\"z is even\")\n} else {\n  print(\"z is odd\")\n}\n\n[1] \"z is odd\"\n\n\n\n\nNote, that SQLite does not support the “normal” if statement. However, the same can be achieved with the case when statement.\n\n-- SQLite flavor\nselect\n  price\n  ,  case when price &gt;= 500 then 'high-price' else 'low-price' end as price_categ\nfrom\n  (select 750 as price)\nlimit 5;\n\n\n1 records\n\n\nprice\nprice_categ\n\n\n\n\n750\nhigh-price"
  },
  {
    "objectID": "data-wrangling/choices.html#the-if-statement",
    "href": "data-wrangling/choices.html#the-if-statement",
    "title": "Choices",
    "section": "",
    "text": "Choices enable you to run different code based on conditions / input. The most basic form of choices is the if statement.\n\nPythonRSQL\n\n\n\nz = 4\nif z % 2 == 0:\n    print(\"z is even\")\nelse:\n    print(\"z is uneven\")\n\nz is even\n\n\n\n\nin one line\n\nz = 4\nif (z %% 2 == 0) print(\"z is even\") else print(\"z is odd\")\n\n[1] \"z is even\"\n\n\nin several lines\n\nz = 5\nif (z %% 2 == 0) {\n  print(\"z is even\")\n} else {\n  print(\"z is odd\")\n}\n\n[1] \"z is odd\"\n\n\n\n\nNote, that SQLite does not support the “normal” if statement. However, the same can be achieved with the case when statement.\n\n-- SQLite flavor\nselect\n  price\n  ,  case when price &gt;= 500 then 'high-price' else 'low-price' end as price_categ\nfrom\n  (select 750 as price)\nlimit 5;\n\n\n1 records\n\n\nprice\nprice_categ\n\n\n\n\n750\nhigh-price"
  },
  {
    "objectID": "data-wrangling/choices.html#the-elif-statement",
    "href": "data-wrangling/choices.html#the-elif-statement",
    "title": "Choices",
    "section": "The elif statement",
    "text": "The elif statement\nFor more complex choices, the elif or else if statement can be used.\n\n\n\n\n\n\nCaution\n\n\n\nKeep in mind, that only the line of code in the first true condition will run. The order of conditions matters!\n\n\n\nPythonRSQL\n\n\n\ndef get_price_categ(price):\n  categ = str()\n  if price &gt; 1000:\n    categ = \"high\"\n  elif price &gt; 700:\n    categ = \"middle\"\n  elif price &gt; 300:\n    categ = \"low\"\n  elif price &gt; 0:\n    categ = \"super cheap\"\n  elif price == 0:\n    categ = \"free\"\n  else:\n    categ = \"there must be an error\"\n  return categ\n\nget_price_categ(750)  \n\n'middle'\n\n\n\n\nThe if in R works specifically or scalars.\n\nget_price_categ &lt;- function(price) {\n  categ &lt;- character()\n  if (price &gt; 1000) {\n    categ &lt;- \"high\"\n  } else if (price &gt; 700) {\n    categ &lt;- \"middle\"\n  } else if (price &gt; 300) {\n    categ &lt;- \"low\"\n  } else if (price &gt; 0) {\n    categ &lt;- \"super cheap\"\n  } else if (price == 0) {\n    categ &lt;- \"free\"\n  } else {\n    categ &lt;- \"there must be an error\"\n  }\n  return(categ)\n}\n\nget_price_categ(17)\n\n[1] \"super cheap\"\n\n\n\n\nNote, that many SQL flavors do not have a classic elif, but the case when can be used as an elif like the following:\n\n-- SQLite flavor\nselect \n  price,\n  case \n    when price &gt; 1000 then 'high'\n    when price &gt; 700 then 'middle'\n    when price &gt; 300 then 'low'\n    when price &gt; 0 then 'super cheap'\n    when price = 0 then 'free'\n    else 'there must be an error'\n  end as price_category\nfrom\n  (select 750 as price) as subquery;\n\n\n1 records\n\n\nprice\nprice_category\n\n\n\n\n750\nmiddle"
  },
  {
    "objectID": "data-wrangling/choices.html#the-if-elif-case-when-statement-for-vectors-columns",
    "href": "data-wrangling/choices.html#the-if-elif-case-when-statement-for-vectors-columns",
    "title": "Choices",
    "section": "The if / elif / case when statement for vectors / columns",
    "text": "The if / elif / case when statement for vectors / columns\nA classic task in data cleaning is changing the values of a column in a df to a different value, based on conditions.\n\nPythonRSQL\n\n\n\nimport pandas as pd\n\ndf = orders[['quantity']].copy()\ndf['order_size'] = df['quantity'].apply(lambda x: 'large' if x &gt;= 3 else 'small')\ndf = df.iloc[0:5]\n\nprint(df)\n\n\n\nThe ifelse in R works specifically for vectors.\n\ndf &lt;- orders %&gt;% \n  select(quantity) %&gt;% \n  mutate(order_size = ifelse(quantity &gt; 3, \"large\", \"small\")) %&gt;% \n  slice(1:5)\ndf\n\n# A tibble: 5 × 2\n  quantity order_size\n     &lt;dbl&gt; &lt;chr&gt;     \n1        2 small     \n2        5 large     \n3        3 small     \n4        2 small     \n5        1 small     \n\n\nFor more conditions, the dplyr::case_when feels more natural and improves readability, especially when having several conditions\n\ndf &lt;- orders %&gt;%\n  select(quantity) %&gt;% \n  mutate(\n    quantity_categ = case_when(\n      quantity == 1 ~ \"small\",\n      quantity %in% c(2, 3) ~ \"middle\",\n      quantity &gt; 3 ~ \"large\",\n      TRUE ~ \"there must be sth wrong\" # this line is like an \"else\"\n    )\n  ) %&gt;% \n  slice(1:5)\ndf\n\n# A tibble: 5 × 2\n  quantity quantity_categ\n     &lt;dbl&gt; &lt;chr&gt;         \n1        2 middle        \n2        5 large         \n3        3 middle        \n4        2 middle        \n5        1 small         \n\n\n\n\n\n-- SQLite flavor\nselect quantity,\n    case\n        when quantity = 1 then 'small'\n        when quantity IN (2, 3) then 'middle'\n        when quantity &gt; 3 then 'large'\n        else 'there must be sth wrong'\n    end as quantity_categ\nfrom orders\nlimit 5;\n\n\n5 records\n\n\nquantity\nquantity_categ\n\n\n\n\n2\nmiddle\n\n\n5\nlarge\n\n\n3\nmiddle\n\n\n2\nmiddle\n\n\n1\nsmall"
  },
  {
    "objectID": "data-wrangling/choices.html#working-with-and-and-or",
    "href": "data-wrangling/choices.html#working-with-and-and-or",
    "title": "Choices",
    "section": "Working with and and or",
    "text": "Working with and and or\nFor more complex choices, where several combinations of conditions are necessary, the conditions are comibned using and, or and brackets to group conditions together.\n\nPythonRSQL\n\n\n\n# Sample data\nage = 25\nincome = 35000\nstudent = True\n\n# Logical conditions\nis_young_adult = 18 &lt;= age &lt;= 30    # Age between 18 and 30\nhas_low_income = income &lt; 30000             # Income below 30,000\nis_student = student                       # Is a student\n\n# Combining conditions with 'and', 'or', and brackets\nif is_young_adult and (has_low_income or is_student):\n    print(\"Financial support applicable\")\nelse:\n    print(\"Financial support not applicable\")\n\nFinancial support applicable\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor performance improvement, it may be useful to use && instead of &. The && operator will only run the condition(s) on the right handside, if the condition on the left handside is TRUE, saving unnecessary computaion.\n\n\n\n# Sample data\nage &lt;- 25\nincome &lt;- 35000\nstudent &lt;- TRUE\n\n# Logical conditions\nis_young_adult &lt;- age &gt;= 18 & age &lt;= 30   # Age between 18 and 30\nhas_low_income &lt;- income &lt; 30000          # Income below 30,000\nis_student &lt;- student                     # Is a student\n\n# Combining conditions with 'and', 'or', and brackets\nif (is_young_adult & (has_low_income | is_student)) {\n  print(\"Financial support applicable\")\n} else {\n  print(\"Financial support not applicable\")\n}\n\n[1] \"Financial support applicable\"\n\n\n\n\n\n-- sample data\nwith data as (\n  select 25 as age, 35000 as income, 1 as student\n)\n-- conditional logic using case statement\nselect\n  case\n    when age &gt;= 18 and age &lt;= 30 and (income &lt; 30000 or student = 1) then 'financial support applicable'\n    else 'financial support not applicable'\n    end as support_status\nfrom\n  data;\n\n\n1 records\n\n\nsupport_status\n\n\n\n\nfinancial support applicable"
  },
  {
    "objectID": "data-wrangling/choices.html#the-switch-statement",
    "href": "data-wrangling/choices.html#the-switch-statement",
    "title": "Choices",
    "section": "The switch statement",
    "text": "The switch statement\nThe switch statement is especially useful for getting values from a dictionary-like construct with key and value.\n\nR\n\n\n\nget_height &lt;- function(x) {\n  switch(x,\n    \"eiffel tower\" = 330,\n    \"Burj Khalifa\" = 828,\n    \"Shanghai Tower\" = 632,\n    stop(\"not found\") # always include a \"not found\" for debugging purposes\n  )\n}\nget_height(\"Shanghai Tower\")\n\n[1] 632"
  },
  {
    "objectID": "data-wrangling/window_functions.html",
    "href": "data-wrangling/window_functions.html",
    "title": "Window functions",
    "section": "",
    "text": "For advanced queries, window functions are very useful to perform calculations based on partition, order and function in a set of rows. They allow to put the current row into context with other rows and perform tasks such as ranking, aggregating, and using analytic functions.\n\n\n\nRanking Functions: Assign e.g. a row number or a rank to a row based on defined criteria. Examples are rank, dense_rank, row_number etc.\nAggregate Functions: Aggregate values over the defined partition avg, sum, count.\nAnalytic Functions: Operate on a range of rows around the current row. Examples are lead, lag, first_value, last_value, nth_value, ntile etc.\n\nAs always, an example says more than a thousand words:"
  },
  {
    "objectID": "data-wrangling/window_functions.html#introduction",
    "href": "data-wrangling/window_functions.html#introduction",
    "title": "Window functions",
    "section": "",
    "text": "For advanced queries, window functions are very useful to perform calculations based on partition, order and function in a set of rows. They allow to put the current row into context with other rows and perform tasks such as ranking, aggregating, and using analytic functions.\n\n\n\nRanking Functions: Assign e.g. a row number or a rank to a row based on defined criteria. Examples are rank, dense_rank, row_number etc.\nAggregate Functions: Aggregate values over the defined partition avg, sum, count.\nAnalytic Functions: Operate on a range of rows around the current row. Examples are lead, lag, first_value, last_value, nth_value, ntile etc.\n\nAs always, an example says more than a thousand words:"
  },
  {
    "objectID": "data-wrangling/window_functions.html#ranking-functions",
    "href": "data-wrangling/window_functions.html#ranking-functions",
    "title": "Window functions",
    "section": "Ranking Functions",
    "text": "Ranking Functions\nAt first glance, it may seem, that the ranking functions all produce the same results. When taking a closer look we notice, that there are slight differences. The row_number will assign a unique value to each row in the window, whereas rank will assign the same value for the same criteria; as in the example below, if there were two values of total_cost that were exactly the same within the window, the rank function would assign the same value (e.g. both 2) and then skip one value (i.e. skip 3 and proceed with 4). The dense_rank is like the rank but without skipping the preceding value.\n\nSQLPythonR\n\n\n\nwith sales as (\n  select\n    c.customer_name  \n    , p.product_name\n    , p.price\n    , o.quantity\n    , p.price * o.quantity as total_cost\n    , order_date\n    , strftime('%Y', order_date) || '-' || strftime('%W', order_date) as yyyy_ww\n    \n  from \n    orders as o\n  inner join products as p on\n    p.product_id = o.product_id\n  inner join customers as c on\n   c.customer_id = o.customer_id\n)\n\nselect\n  *\n  , row_number() over(partition by yyyy_ww order by price desc) as rn\n  , rank() over(partition by yyyy_ww order by price desc) as rank\n  , dense_rank() over(partition by yyyy_ww order by price desc) as dense_rank\nfrom\n  sales\norder by\n  yyyy_ww asc, rn asc\nlimit 15 offset 4;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_name\nproduct_name\nprice\nquantity\ntotal_cost\norder_date\nyyyy_ww\nrn\nrank\ndense_rank\n\n\n\n\nChristine Holt\nact\n422.40\n3\n1267.20\n2023-01-05\n2023-01\n3\n3\n3\n\n\nJodi Brown\nteach\n245.39\n3\n736.17\n2023-01-03\n2023-01\n4\n4\n4\n\n\nDanny Rodriguez\ndrive\n162.04\n4\n648.16\n2023-01-06\n2023-01\n5\n5\n5\n\n\nEric Davis\nrecent\n135.03\n1\n135.03\n2023-01-05\n2023-01\n6\n6\n6\n\n\nRobert Strickland\nwe\n119.74\n1\n119.74\n2023-01-06\n2023-01\n7\n7\n7\n\n\nBrittany Johnson\nthere\n912.52\n1\n912.52\n2023-01-09\n2023-02\n1\n1\n1\n\n\nSarah Kennedy\nthere\n912.52\n4\n3650.08\n2023-01-12\n2023-02\n2\n1\n1\n\n\nTerry Morgan\nthere\n912.52\n4\n3650.08\n2023-01-10\n2023-02\n3\n1\n1\n\n\nAmanda Johnson\nable\n730.41\n3\n2191.23\n2023-01-10\n2023-02\n4\n4\n2\n\n\nRobert Strickland\ntravel\n666.50\n2\n1333.00\n2023-01-09\n2023-02\n5\n5\n3\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nmerged_df = pd.merge(orders, products, how='inner', on='product_id')\nmerged_df = pd.merge(merged_df, customers, how='inner', on='customer_id')\n\n# Calculating total_cost and yyyy_ww\nmerged_df['total_cost'] = merged_df['price'] * merged_df['quantity']\nmerged_df['yyyy_ww'] = merged_df['order_date'].dt.strftime('%Y-%W')\n\nmerged_df['rn'] = merged_df.groupby('yyyy_ww')['price'].transform(lambda x: x.rank(method='first', ascending=False))\nmerged_df['rank'] = merged_df.groupby('yyyy_ww')['price'].rank(method='min', ascending=False)\nmerged_df['dense_rank'] = merged_df.groupby('yyyy_ww')['price'].rank(method='dense', ascending=False)\n\n# Selecting specific columns\nselected_columns = [\n    'customer_name', 'product_name', 'price', 'quantity', 'order_date',\n    'rn', 'rank', 'dense_rank'\n]\n\nresult_df = merged_df.sort_values(by=['yyyy_ww', 'rn']).head(15)[selected_columns].iloc[4:15]\n\nprint(result_df)\n\n         customer_name product_name   price  ...   rn rank  dense_rank\n191     Christine Holt          act  422.40  ...  3.0  3.0         3.0\n94          Jodi Brown        teach  245.39  ...  4.0  4.0         4.0\n336    Danny Rodriguez        drive  162.04  ...  5.0  5.0         5.0\n474         Eric Davis       recent  135.03  ...  6.0  6.0         6.0\n398  Robert Strickland           we  119.74  ...  7.0  7.0         7.0\n105       Terry Morgan        there  912.52  ...  1.0  1.0         1.0\n159      Sarah Kennedy        there  912.52  ...  2.0  1.0         1.0\n331   Brittany Johnson        there  912.52  ...  3.0  1.0         1.0\n416     Amanda Johnson         able  730.41  ...  4.0  4.0         2.0\n401  Robert Strickland       travel  666.50  ...  5.0  5.0         3.0\n344       Aaron Miller         must  657.16  ...  6.0  6.0         4.0\n\n[11 rows x 8 columns]\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\n\nsales &lt;- orders %&gt;%\n  inner_join(products, by = \"product_id\") %&gt;%\n  inner_join(customers, by = \"customer_id\") %&gt;%\n  mutate(\n    total_cost = price * quantity,\n    yyyy_ww = paste0(strftime(order_date, \"%Y\"), \"-\", strftime(order_date, \"%W\"))\n  )\n\nsales &lt;- sales %&gt;%\n  arrange(yyyy_ww, desc(price)) %&gt;%\n  group_by(yyyy_ww) %&gt;%\n  mutate(\n    rn = row_number(desc(price)),\n    rank = rank(desc(price)),\n    dense_rank = dense_rank(desc(price))\n  ) %&gt;%\n  ungroup() %&gt;% # dplyr uses the ungroup() as pendant to window functions\n  select( rn, rank, dense_rank, customer_name, product_name,\n          price, quantity, order_date,\n  ) %&gt;%\n  slice(5:15)\nsales\n\n# A tibble: 11 × 8\n      rn  rank dense_rank customer_name     product_name price quantity\n   &lt;int&gt; &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1     3     3          3 Christine Holt    act           422.        3\n 2     4     4          4 Jodi Brown        teach         245.        3\n 3     5     5          5 Danny Rodriguez   drive         162.        4\n 4     6     6          6 Eric Davis        recent        135.        1\n 5     7     7          7 Robert Strickland we            120.        1\n 6     1     2          1 Brittany Johnson  there         913.        1\n 7     2     2          1 Sarah Kennedy     there         913.        4\n 8     3     2          1 Terry Morgan      there         913.        4\n 9     4     4          2 Amanda Johnson    able          730.        3\n10     5     5          3 Robert Strickland travel        666.        2\n11     6     6          4 Aaron Miller      must          657.        5\n# ℹ 1 more variable: order_date &lt;dttm&gt;"
  },
  {
    "objectID": "data-wrangling/window_functions.html#aggregating-functions",
    "href": "data-wrangling/window_functions.html#aggregating-functions",
    "title": "Window functions",
    "section": "Aggregating Functions",
    "text": "Aggregating Functions\nThey are very useful to aggregate data on specified criteria using a window. The window can be aggregated using the partition clause and ordered by the order clause. Finally, a window can be set using rows between X preceding and Y following with X and Y being integers or unbounded, if we want the window to start from start to the end. It is also possible to set a range instead of rows\n\nSQLPythonR\n\n\n\nwith sales as (\n  select\n    c.customer_id\n    , p.product_name\n    , p.price\n    , o.quantity\n    , p.price * o.quantity as total_cost\n    , order_date\n  from \n    orders as o\n  inner join products as p on\n    p.product_id = o.product_id\n  inner join customers as c on\n    c.customer_id = o.customer_id\n)\n\nselect\n  *\n  , avg(total_cost) over(order by order_date rows between 1 preceding and 1 following) as avg_3_rows\n  , sum(total_cost) over(order by order_date rows unbounded preceding) as running_sum\n  , count() over(partition by customer_id) as orders_per_cust\nfrom\n  sales\norder by\n  order_date\n  \n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nproduct_name\nprice\nquantity\ntotal_cost\norder_date\navg_3_rows\nrunning_sum\norders_per_cust\n\n\n\n\n0b18417b-543d-429c-a468-1592aea8c1a2\nconference\n572.67\n4\n2290.68\n2023-01-01\n1539.9650\n2290.68\n3\n\n\n0e683879-4049-4382-b977-965a06daa874\nauthor\n789.25\n1\n789.25\n2023-01-01\n1272.0333\n3079.93\n2\n\n\nc9b595f2-b44b-4b02-ad87-1fba8404074d\nteach\n245.39\n3\n736.17\n2023-01-03\n1091.6067\n3816.10\n3\n\n\nc9b595f2-b44b-4b02-ad87-1fba8404074d\nday\n874.70\n2\n1749.40\n2023-01-04\n873.5333\n5565.50\n3\n\n\n2c1823ce-2ca8-440f-8b47-d00873dd6e2c\nrecent\n135.03\n1\n135.03\n2023-01-05\n1050.5433\n5700.53\n2\n\n\ne3c0eb04-ba9b-4fe5-89b2-d099e193d04d\nact\n422.40\n3\n1267.20\n2023-01-05\n507.3233\n6967.73\n2\n\n\n6680325a-df9b-407c-b3d1-0a19a96ca118\nwe\n119.74\n1\n119.74\n2023-01-06\n678.3667\n7087.47\n4\n\n\nb7887f71-9683-470a-8718-68c49cb4bb18\ndrive\n162.04\n4\n648.16\n2023-01-06\n402.5133\n7735.63\n2\n\n\n0e683879-4049-4382-b977-965a06daa874\nfast\n439.64\n1\n439.64\n2023-01-08\n806.9333\n8175.27\n2\n\n\n6680325a-df9b-407c-b3d1-0a19a96ca118\ntravel\n666.50\n2\n1333.00\n2023-01-09\n1686.1467\n9508.27\n4\n\n\n\n\n\n\n\n\nimport pandas as pd\n\nsales = pd.merge(orders, products, on='product_id').merge(customers, on='customer_id')\nsales['total_cost'] = sales['price'] * sales['quantity']\nsales = sales.sort_values('order_date')\nsales['avg_3_rows'] = sales['total_cost'].rolling(window=3, min_periods=1).mean()\nsales['running_sum'] = sales['total_cost'].cumsum()\nsales['orders_per_cust'] = sales.groupby('customer_id')['customer_id'].transform('count')\nresult = sales[['customer_id', 'product_name', 'price', 'quantity', 'total_cost', 'order_date', 'avg_3_rows', 'running_sum', 'orders_per_cust']]\nresult = result.sort_values('order_date').reset_index(drop=True)\nprint(result)\n\n                              customer_id  ... orders_per_cust\n0    0b18417b-543d-429c-a468-1592aea8c1a2  ...               3\n1    0e683879-4049-4382-b977-965a06daa874  ...               2\n2    c9b595f2-b44b-4b02-ad87-1fba8404074d  ...               3\n3    c9b595f2-b44b-4b02-ad87-1fba8404074d  ...               3\n4    e3c0eb04-ba9b-4fe5-89b2-d099e193d04d  ...               2\n..                                    ...  ...             ...\n495  5b4e2a6e-dab7-415b-badb-1e4ef26155ac  ...               4\n496  96e4be4c-42a3-4f62-ba5d-46a022883c83  ...               1\n497  b0f2cebf-a461-4135-886a-10e97cc2dac6  ...               3\n498  fd6d1332-e821-4d2b-87e0-2dc1c7d734e7  ...               2\n499  0b6bc6fb-4e69-4fe0-8bd4-aee20da0116b  ...               1\n\n[500 rows x 9 columns]\n\n\n\n\nNote that the results around the first and last rows may differ compared to the sql junk, since the window functions are differently implemented in sql and R.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ tibble  3.2.1     ✔ stringr 1.5.1\n✔ readr   2.1.5     ✔ forcats 1.0.0\n✔ purrr   1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::modify() masks renv::modify()\n\nlibrary(slider)\n\nsales &lt;- orders %&gt;%\n  inner_join(products, by = \"product_id\") %&gt;%\n  inner_join(customers, by = \"customer_id\") %&gt;%\n  mutate(total_cost = price * quantity) %&gt;% \n  select(\n    customer_id,\n    product_name,\n    price,\n    quantity,\n    total_cost,\n    order_date\n  )\n\nresult &lt;- sales %&gt;%\n  arrange(order_date) %&gt;%\n  mutate(\n    avg_3_rows = slide_dbl(total_cost, mean, .before = 1, .after = 1, .complete = TRUE),\n    running_sum = cumsum(total_cost),\n    orders_per_cust = n()\n  ) %&gt;%\n  group_by(customer_id) %&gt;%\n  mutate(\n    orders_per_cust = n()\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(order_date)\n\nresult &lt;- sales %&gt;%\n  group_by(customer_id) %&gt;%\n  arrange(order_date) %&gt;% \n  mutate(\n    avg_3_rows = slide_dbl(total_cost, mean, .before = 1, .after = 1, .complete = TRUE),\n    running_sum = cumsum(total_cost),\n    orders_per_cust = n()\n  )\n\nresult\n\n# A tibble: 500 × 9\n# Groups:   customer_id [239]\n   customer_id        product_name price quantity total_cost order_date         \n   &lt;chr&gt;              &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dttm&gt;             \n 1 0b18417b-543d-429… conference    573.        4      2291. 2023-01-01 00:00:00\n 2 0e683879-4049-438… author        789.        1       789. 2023-01-01 00:00:00\n 3 c9b595f2-b44b-4b0… teach         245.        3       736. 2023-01-03 00:00:00\n 4 c9b595f2-b44b-4b0… day           875.        2      1749. 2023-01-04 00:00:00\n 5 2c1823ce-2ca8-440… recent        135.        1       135. 2023-01-05 00:00:00\n 6 e3c0eb04-ba9b-4fe… act           422.        3      1267. 2023-01-05 00:00:00\n 7 6680325a-df9b-407… we            120.        1       120. 2023-01-06 00:00:00\n 8 b7887f71-9683-470… drive         162.        4       648. 2023-01-06 00:00:00\n 9 0e683879-4049-438… fast          440.        1       440. 2023-01-08 00:00:00\n10 e32da01f-81f0-4da… there         913.        1       913. 2023-01-09 00:00:00\n# ℹ 490 more rows\n# ℹ 3 more variables: avg_3_rows &lt;dbl&gt;, running_sum &lt;dbl&gt;,\n#   orders_per_cust &lt;int&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to coding-snippet",
    "section": "",
    "text": "Welcome to coding-snippet! We offer an open source collection of data science snippets in various programming languages like Python, R and SQL that are relevant for data-related problems. Our platform is specifically designed for data enthusiasts who are tired of scanning the internet over and over again for the very same code snippets they use in their daily data analysis tasks.\n\n\n\nCurated Code Snippets: We documented a wide range of code snippets for problems that come across a data analyst’s / data scientist’s work day all the time. Copy, paste and adjust the snippets according to your problem.\nProblem Oriented Approach: As data scientists, we jump between coding and query languages all the time; writing a database query in SQL, adding another data source from a python-based web-scraper and visualizing the beautifully aggregated data as a plot in R ggplot. We noticed, that most coders document their snippets in a language oriented manner, having e.g. one document per language. We believe in the problem oriented approach, where the snippets are organized according to the problem.\nPlatform for Collaboration: Become a collaborator on our Github repository and contribute to improving the quality and amount of the coding snippet base. We value your input and are always open for constructive criticism.\n\n\n\n\nWe believe, that a well-organized documentation of the different problems and tools helps the data science community to work more efficiently and helps rookies to get started much faster. Feel free for reaching out to us for an informal (virtual or in-person) coffee.\n\n\n\nFind us online and on LinkedIn for the latest updates, discussions, and insights.\nThank you for taking part in our community and sharing our knowledge base with others!"
  },
  {
    "objectID": "index.html#about-us",
    "href": "index.html#about-us",
    "title": "Welcome to coding-snippet",
    "section": "",
    "text": "Welcome to coding-snippet! We offer an open source collection of data science snippets in various programming languages like Python, R and SQL that are relevant for data-related problems. Our platform is specifically designed for data enthusiasts who are tired of scanning the internet over and over again for the very same code snippets they use in their daily data analysis tasks.\n\n\n\nCurated Code Snippets: We documented a wide range of code snippets for problems that come across a data analyst’s / data scientist’s work day all the time. Copy, paste and adjust the snippets according to your problem.\nProblem Oriented Approach: As data scientists, we jump between coding and query languages all the time; writing a database query in SQL, adding another data source from a python-based web-scraper and visualizing the beautifully aggregated data as a plot in R ggplot. We noticed, that most coders document their snippets in a language oriented manner, having e.g. one document per language. We believe in the problem oriented approach, where the snippets are organized according to the problem.\nPlatform for Collaboration: Become a collaborator on our Github repository and contribute to improving the quality and amount of the coding snippet base. We value your input and are always open for constructive criticism.\n\n\n\n\nWe believe, that a well-organized documentation of the different problems and tools helps the data science community to work more efficiently and helps rookies to get started much faster. Feel free for reaching out to us for an informal (virtual or in-person) coffee.\n\n\n\nFind us online and on LinkedIn for the latest updates, discussions, and insights.\nThank you for taking part in our community and sharing our knowledge base with others!"
  }
]